# Training Neural Networks

## Instructor 

### Luis Serrano

Now that you know so much about neural networks, you can continue to the next step, Training the network.

Please say hello to Luis again!



## Training Optimization 

""" 

Somtimes we go out there and train on ourselves and find out that nothing works as planned. 

Why? Because there are many things that can fail.

Our architecture can be poorly chosen

Our data can be noisy

Our model couold maybe be taking years to run and need it to run faster. 

We need to learn ways to optimize the training of our models and this is what we'll do next. 

"""



## Testing

In order to find out which model is better , we introduce the concept of training and testing sets. 

We will train our models in the training set without looking at the testing set., and then evaluate the results on that testing to see how we did. 


Whenever we can choose between a simple model that does the job and a compicated model that may do the job a little bit better, we always try to go for the simpler model. 



## Overfitting and Underfitting 

 Uderfitting : trying a solution that is too simple and won't do the job. 

(high bias)


 Overfitting : overly complicated and it will lead to bad solutions and extra complexity when we can use a much simpler solution instead. 
 
(high variance) 




## Early Stopping 

### Model Complexity Graph

#### Early stopping algorithm 
We degrade in descent until the testing error stops decreasing and start to increase. At that moment, we stop.



## Regularization 

In the coordination, there are (1,1) and (-1,-1)

Which gives a smaller error?

* x1 + x2

* 10x1 + 10x2


The prediction is a sigmoid of the linear funtion. 

So for the first case :

σ(1+1) = 0.88

σ(-1-1) = 0.12

For the second case : 

σ(10+10) = 0.999999999979

σ(-10-10) = 0.00000000021

So 10x1 + 10x2 gives us small error.

But it's too certain.

L1 regularization : 

λ(|w1|+...+|wn|) 

Good for feature selection

L1 tend to end up with sparce vectors. Small weights will tend to go to zero. 


L2 regularization : 

λ(w1^2+...+wn^2)

Normally better for training models 

L2 tends not to favor sparse vectors since it tries to maintain all the wieghts homogeneously small.




## Dropout

Sometimes one part of the network has very large weights and it ends up dominating all the training,  while other part of the network doesn't really play much of a role so it doesn't get trained. 


So we randomly turn off some of the nodes and say , you shall not pass through here .

Probability each node will be dropped = 0.2 



## Local Minima

Doing gradient descent may stuck in local minima.


## Random Restart 

We start from a few different random places and do gradient descent from all of them. 

It will increase the probability that we'll get to the global minumum, or  at least a pretty good local minimum.


## Vanishing Gradient 

On the sigmoid function, the curve gets pretty flat on the sides. So if we calculate the derivative at a point way at the right or way at the left, this derivative is almost zero.  This is not good cause a deriviative is what tell us in what to move. 

This gets even worse in most linear perceptrons.

Derivative of the error function with respect to a weight was the product of all the derivatives calcuated at the nodes in the corresponding path to the output.  All these derivatives are derivatives as a sigmoid function, so they're small and the product of a bunch of small numbers is tiny. 

This makes the training difficult because basically gradient descent gives us very tiny changes to make on the weights. 



## Other Activation Functions 

The best way to fix vanishing gradient is to change the activation funtion. 

__Hyperbolic Tangent (tanh(x))

__Rectified Linear Unit (ReLU)



## Batch vs Stochastic Gradient Descent 

With gradient descnet, each step is called an epoch.

In each epoch : 

* we take our input namly all of our data
* run it throught the entire neural network 
* find our predictions
* calculatie the error
* back-propagate this error 
* in order to update the weights in the neural network.

This is single step. And if we had to do many steps, you can image how this would take a long ime and lots of computiing power. 

Q : Do we need to plug in all our data everytime we take a step?

If the data is well distributed, it's almost like a small subset of it would give us a pretty good idea of what the gradient would be. 

Maybe it's not the best estimate for the gradient but it's quick. 

### Stochasitc gradient descnet 
* We take small subsets of data
* Run them throught the neural network
* Calculate the gradient of the error function based on those points and then momve one step in that direction. 

* Split the data into several batches
* Take the points in the first batch and run them through the neural network
* Calculate the error and its gradient and back-propagate to update the weights 
* Take the poinst in the second batch and do the same thing 



## Learning Rate Decay 

Too big : taking huge steps which could be fast at the beginning but may miss the minimum and keep going

Too small : make steady steps and have a better chance of arriving to your local minimum.


### Decreasing Learning Rate

Rule : 

* If steep : long steps 
* If plain : small steps 



## Momentum 

Momentum is a constant beta between 0 and 1 that attaches to the steps as follows : 

* the previous step gets multiplied by 1,
* the one before, by beta
* the one before, by beta squared
* the one before, by beta cubed 

In this way, the steps that happend a long time ago will matter less than ones that happend recently. 



## Error Functions Around the World






