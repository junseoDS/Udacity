# Sentiment Analysis


## Introducing Andrew Trask

PhD student at Univ of Oxford

Deep learning for NLP

Author of grokking deep learning 



## Meet Andrew

> Deep learning is that take what you know and predict what you want to know using network.
>
> Sentiment classification of the classification of whether or not a section of human generated text is positive or negative. 
>
> Framing a problem , so the network can be successful in discovering correlation between your input and your output data. 
>
> Transform our textual input data into numerical form in such a way that the neural network can easily discover the correlation. 
>
> This tutorial is going to be broken into 6 differnt projects
>
> * Curating your dataset
> * Coming up with a predictive theory for where we think the correlation exists in our dataset
> * Validate the theory 
> * Transfrom it into your input output data 
> * Iterate several times
> * Try to increase the amount of correlation that our neural networks are able to find in a smaller period of time.
> * At the end, tear apart our neural networks and check out the weights to really understand what's going on when we're back propagating, to try to solve assignment .


__Note__
For a 40% discount on [Andrew's book](https://www.manning.com/books/grokking-deep-learning), use coupon code traskud17 .



## Materials


### Materials

As you follow along this lesson, it's extremely important that you open the Jupyter notebook and attempt the exercises. Much of the value in this experience will come from seeing how your solution is different from Andrew's and playing around with the code in your own way. Make this lesson count!


### Workspace

The best way to open the notebook is to click [here](https://classroom.udacity.com/nanodegrees/nd101/parts/94643112-2cab-46f8-a5be-1b6e4fa7a211/modules/c39246ce-e91f-473d-a2d4-43524786ffbd/lessons/47d78191-5dea-420f-8723-f5f3fd7a148a/concepts/3cc08c9f-50e9-4650-a67e-179a3cbb9dcf), which will open it in a new window. We recommend you to work on the notebook in that window, and watch the videos in this one. You can also get to the notebook by clicking the "Next" button in the classroom.

If you want to download the notebooks yourself, you can clone them from [our GitHub repository](https://github.com/udacity/deep-learning-v2-pytorch), and navigate to the sentiment-analysis-network folder.

> You can either download the repository with git clone https://github.com/udacity/deep-learning-v2-pytorch.git, or download it as an archive file from [this link](https://github.com/udacity/deep-learning-v2-pytorch/archive/master.zip).


This lesson uses the following files:

* Sentiment_Classification_Projects.ipynb - a notebook you will use to following along and work on lesson mini projects.
* Sentiment_Classification_Solutions.ipynb - a notebook that includes Andrew’s solutions to the lesson projects, which you can use for reference
* A notebook for the solution for each mini project.
* reviews.txt - a collection of 25 thousand movie reviews
* labels.txt - positive/negative sentiment labels for the associated reviews in reviews.txt

__Note__ : the notebooks for these lessons have been updated since the videos were recorded. In most cases that just means your notebook will contain more hints and explanatory text than what you see in the videos, but there may be some minor differences in the code as well. With these changes, you still will be able to follow along with the lessons, and should have an easier time understanding the project material.


### Solutions
If you need help, feel free to look at the solutions in the same folder.





## Transforming Text into Numbers




## Mini Project 2


### Instructions


In the following mini project, you'll convert the inputs and outputs of the dataset into numbers. Namely, you will convert each review string into a vector, and each label into a 0 or 1. You’ll need to make a few additions to the notebook, but the main work will be implementing two functions, whose signatures are shown below:


    def update_input_layer(review):
        """ Modify the global layer_0 to represent the vector form of review.
        The element at a given index of layer_0 should represent \
        how many times the given word occurs in the review.
        Args:
            review(string) - the string of the review
        Returns:
            None
        """
        global layer_0
        # clear out previous state, reset the layer to be all 0s
        layer_0 *= 0
        ## Your code here
        pass



    def get_target_for_label(label):
        """Convert a label to `0` or `1`.
        Args:
            label(string) - Either "POSITIVE" or "NEGATIVE".
        Returns:
            `0` or `1`.
        """
        pass






## Better Weight Initializaion Strategy 



### Better Weight Initialization Strategy

In the last video, Andrew Trask built a neural network and initialized the weights according to the function init_network:

    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
            # Set number of nodes in input, hidden and output layers.
            self.input_nodes = input_nodes
            self.hidden_nodes = hidden_nodes
            self.output_nodes = output_nodes

            # Store the learning rate
            self.learning_rate = learning_rate

            # Initialize weights

            # These are the weights between the input layer and the hidden layer.
            self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))

            # These are the weights between the hidden layer and the output layer.
            self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, 
                                                    (self.hidden_nodes, self.output_nodes))

            # The input layer, a two-dimensional matrix with shape 1 x input_nodes
            self.layer_0 = np.zeros((1,input_nodes))


Here, I'd like you to take note of the line 

    self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes)) 

this is one possible initialization strategy, but there is actually a better way to initialize these weights that I will briefly go over in this concept.

### Different Initialization, Better Accuracy

You'll learn more about weight initialization, later on in this program. Suffice to say that the general rule for setting the weights in a neural network is to set them to be close to zero without being too small.


> Good practice is to start your weights in the range of [-y, y] where
>
> y=1/sqrt{n}
>
> And n is the number of inputs to a given layer.


In this case, between layers 1 and 2, a better solution would be to define the weights as a function of the number of nodes n in the hidden layer.

So, the initialization code would change; you'd use hidden_nodes**-0.5 rather than output_nodes**-0.5 If you try this out, you should see that you get improved training with a larger learning rate = 0.01 rather than waiting for a learning rate of 0.001.

    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
            # Set number of nodes in input, hidden and output layers.
            self.input_nodes = input_nodes
            self.hidden_nodes = hidden_nodes
            self.output_nodes = output_nodes

            # Store the learning rate
            self.learning_rate = learning_rate

            # Initialize weights

            # These are the weights between the input layer and the hidden layer.
            self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))

            # These are the weights between the hidden layer and the output layer.
            ## NOTE: the difference in the standard deviation of the normal weights
            ## This was changed from `self.output_nodes**-0.5` to `self.hidden_nodes**-0.5`
            self.weights_1_2 = np.random.normal(0.0, self.hidden_nodes**-0.5, 
                                                    (self.hidden_nodes, self.output_nodes))

            # The input layer, a two-dimensional matrix with shape 1 x input_nodes
            self.layer_0 = np.zeros((1,input_nodes))
            
            

### The Code
Andrew will still use the initialization strategy using output_nodes**-0.5, but you can see the solution code for this alternate weight initialization strategy in the [Github repo](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/sentiment-analysis-network/Sentiment_Classification_Solutions_2_Better_Weight_Initialization.ipynb).


## Mini project 5 

### Instructions
Congratulations on making it all the way to the fifth project! In this mini project, you'll dramatically improve the performance of your network using some of the simple ideas Andrew mentioned. By the end, you'll see how your network can train far faster than before! 


.
.
.
.
.
.






## Conclusion

