# Introduction to Neural Networks


## Instructor

Hello and welcome to Introduction to Neural Networks, given by Luis!

Luis was formerly a Machine Learning Engineer at Google. He holds a PhD in mathematics from the University of Michigan, and a Postdoctoral Fellowship at the University of Quebec at Montreal.



## Introduction 

### "What is Deep Learning "

### "What is it used for ? "
> Pretty much everywhere

### Neural Networks
> Neural networks vaguely mimic the process of how the brain operates, with neurons that fire bits of information.



## Classification Problems 1


### Classification Problems
We'll start by defining what we mean by classification problems, and applying it to a simple example.

##### Acceptance at a University

two information : 
* Test
* Grades

__Student 1__
Test : 9/10

Grades : 8/10

Accepted

__Student 2__

Test : 3/10

Grades : 4/10

Rejected

__Student 3__

Test :7/10

Grades : 6/10

???

## Classification Problems 2

"How do we find this line(MODEL) ?"


## Linear Boundaries 

x_1 : Test

x_2 : Grades 

Boundary : A Line

2x_1 + x_2 -18 =0

Score = 2\*Test + Grades - 18

Prediction : 

Score > 0 : Accept

Score < 0 : Reject


In more general case, our boundary will be an equation of the following :

w_1x_1 + w_2x_2 + b = 0 

Wx +b =0

W=(w_1,w_2)

x=(x_1,x_2)

y= label : 0 or 1

Prediction:

y_hat = 1 if Wx +b >= 0

"      = 0 if Wx +b <0



## Higher Dimensions 

### 3 dimension

Boundary : A Plane

w_1x_1 + w_2x_2 + w_3x_3 +b =0

Wx +b =0

Prediciotn :

y_hat = 1 if Wx + b >= 0

"     = 0 if Wx + b < 0



### n-dimensional space 

x_1, x_2, ... x_3

Boundary : n-1 dimensional hyperplane

w_1x_1 + w_2x_2 + ... w_nx_n + b = 0

Wx + b = 0

Prediction :

y_hat = 1 if Wx +b >= 0

  "   = 0 of Wx +b < 0 



## Perceptrons

Wx + b = ∑WiXi + b

### Step Function 

y = 1 if x >= 0

y = 0 if x < 0


## Why "Neural Networks ? "

the reason why they're called neural network is because perceptrons kind of look like neurons in the brain. 




## Perceptrons as Logical Operators

In this lesson, we'll see one of the many great applications of perceptrons. As logical operators! You'll have the chance to create the perceptrons for the most common of these, the AND, OR, and NOT operators. And then, we'll see what to do about the elusive XOR operator. Let's dive in!

### AND Perceptron 

The inputs can be true or false, but the output is only true if both of the inputs are true. 

### Or Perceoptron

OR operatior which returns true if any of its two inputs is true . 


### What are the weights and bias for the AND perceptron?
Set the weights (weight1, weight2) and bias (bias) to values that will correctly determine the AND operation as shown above.
More than one set of values will work!

    import pandas as pd

    # TODO: Set weight1, weight2, and bias
    weight1 = 0.0
    weight2 = 0.0
    bias = 0.0


    # DON'T CHANGE ANYTHING BELOW
    # Inputs and outputs
    test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
    correct_outputs = [False, False, False, True]
    outputs = []

    # Generate and check output
    for test_input, correct_output in zip(test_inputs, correct_outputs):
        linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
        output = int(linear_combination >= 0)
        is_correct_string = 'Yes' if output == correct_output else 'No'
        outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])

    # Print output
    num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
    output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])
    if not num_wrong:
        print('Nice!  You got it all correct.\n')
    else:
        print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
    print(output_frame.to_string(index=False))



### OR Perceptron

he OR perceptron is very similar to an AND perceptron. The OR perceptron has the same line as the AND perceptron, except the line is shifted down. What can you do to the weights and/or bias to achieve this? Use the following AND perceptron to create an OR Perceptron.


### NOT Perceptron
Unlike the other perceptrons we looked at, the NOT operation only cares about one input. The operation returns a 0 if the input is 1 and a 1 if it's a 0. The other inputs to the perceptron are ignored.

In this quiz, you'll set the weights (weight1, weight2) and bias bias to the values that calculate the NOT operation on the second input and ignores the first input.



    import pandas as pd

    # TODO: Set weight1, weight2, and bias
    weight1 = 0.0
    weight2 = 0.0
    bias = 0.0


    # DON'T CHANGE ANYTHING BELOW
    # Inputs and outputs
    test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
    correct_outputs = [True, False, True, False]
    outputs = []

    # Generate and check output
    for test_input, correct_output in zip(test_inputs, correct_outputs):
        linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
        output = int(linear_combination >= 0)
        is_correct_string = 'Yes' if output == correct_output else 'No'
        outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])

    # Print output
    num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
    output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])
    if not num_wrong:
        print('Nice!  You got it all correct.\n')
    else:
        print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
    print(output_frame.to_string(index=False))
    
    
    
### XOR Perceptron

### Quiz: Build an XOR Multi-Layer Perceptron
Now, let's build a multi-layer perceptron from the AND, NOT, and OR perceptrons to create XOR logic!

The neural network below contains 3 perceptrons, A, B, and C. The last one (AND) has been given for you. The input to the neural network is from the first node. The output comes out of the last node.

The multi-layer perceptron below calculates XOR. Each perceptron is a logic operation of AND, OR, and NOT. However, the perceptrons A, B, and C don't indicate their operation. In the following quiz, set the correct operations for the perceptrons to calculate XOR.


A : AND B : OR C : NOT


## Perceptron Trick 

### Perceptron Trick
In the last section you used your logic and your mathematical knowledge to create perceptrons for some of the most common logical operators. In real life, though, we can't be building these perceptrons ourselves. The idea is that we give them the result, and they build themselves. For this, here's a pretty neat trick that will help us.

#### Split Data 

> Does the misclassified point want the line to be closer or farther?
>
> Closer

### Time for some math!
Now that we've learned that the points that are misclassified, want the line to move closer to them, let's do some math. The following video shows a mathematical trick that modifies the equation of the line, so that it comes closer to a particular point.


## Perceptron Algorithm

### Perceptron Algorithm
And now, with the perceptron trick in our hands, we can fully develop the perceptron algorithm! The following video will show you the pseudocode, and in the quiz below, you'll have the chance to code it in Python.

1. Start with random weight : w1,...wn,b

2. For every misclassified point(x1,..xn) :

  2.1 If prefiction =0 :
  
.    -For i = 1...n
    
.     -Change wi = wi + axi

  2.2 If prediction =1 :
    
.    - For i = 1...n 
    
.      - Change wi = wi-axi
      
.    - Change b to b-a


### Coding the Perceptron Algorithm
Time to code! In this quiz, you'll have the chance to implement the perceptron algorithm to separate the following data (given in the file data.csv).

Recall that the perceptron step works as follows. For a point with coordinates (p,q)(p,q), label yy, and prediction given by the equation \hat{y} = step(w_1x_1 + w_2x_2 + b) 

* If the point is correctly classified, do nothing.
* If the point is classified positive, but it has a negative label, subtract \alpha p, \alpha q,αp,αq, and \alphaα from w_1, w_2, and b respectively.
If the point is classified negative, but it has a positive label, add \alpha p, \alpha q,αp,αq, and \alphaα to w_1, w_2, and b respectively.

Then click on test run to graph the solution that the perceptron algorithm gives you. It'll actually draw a set of dotted lines, that show how the algorithm approaches to the best solution, given by the black solid line.

Feel free to play with the parameters of the algorithm (number of epochs, learning rate, and even the randomizing of the initial parameters) to see how your initial conditions can affect the solution!



    import numpy as np
    # Setting the random seed, feel free to change it and see different solutions.
    np.random.seed(42)

    def stepFunction(t):
        if t >= 0:
            return 1
        return 0

    def prediction(X, W, b):
        return stepFunction((np.matmul(X,W)+b)[0])

    # TODO: Fill in the code below to implement the perceptron trick.
    # The function should receive as inputs the data X, the labels y,
    # the weights W (as an array), and the bias b,
    # update the weights and bias W, b, according to the perceptron algorithm,
    # and return W and b.
    def perceptronStep(X, y, W, b, learn_rate = 0.01):
        # Fill in code
        for i in range(len(X)) :
            y_hat= prediction(X[i],W,b)
            if y[i]-y_hat==1:
                W[0] += W[0]*learn_rate
                W[1] += W[1]*learn_rate
                b += b*learn_rate
            elif y[i]-y_hat==-1:
                W[0] -= W[0]*learn_rate
                W[1] -= W[1]*learn_rate
                b -= b*learn_rate
        return W, b

    # This function runs the perceptron algorithm repeatedly on the dataset,
    # and returns a few of the boundary lines obtained in the iterations,
    # for plotting purposes.
    # Feel free to play with the learning rate and the num_epochs,
    # and see your results plotted below.
    def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):
        x_min, x_max = min(X.T[0]), max(X.T[0])
        y_min, y_max = min(X.T[1]), max(X.T[1])
        W = np.array(np.random.rand(2,1))
        b = np.random.rand(1)[0] + x_max
        # These are the solution lines that get plotted below.
        boundary_lines = []
        for i in range(num_epochs):
            # In each epoch, we apply the perceptron step.
            W, b = perceptronStep(X, y, W, b, learn_rate)
            boundary_lines.append((-W[0]/W[1], -b/W[1]))
        return boundary_lines




## Non-Linear Regions

Sometimes the data cannot be separated by just a line.

So what is the next thing after a line?

Maybe circle, two lines, or a curve 

__We need to redefine our perceptron algorithm for a line in a way that it'll generalize to other types of curves.__



## Error Functions 

Error function is simply something that tells us how far we are from the solution. 

= Distance 



## Log-loss Error Function

The error is what's telling us how badly we're doing at the moment and gow far we from an ideal solution.

And if we constantly take steps to decrease the error , then we eventually solvethe problem.

### Discrete vs Continuous

Error function can not be discrete, it should be continuous. 




## Descrete vs Continuous

### Discrete vs Continuous Predictions
We learned that continuous error functions are better than discrete error functions, when it comes to optimizing. For this, we need to switch from discrete to continuous predictions. The next two videos will guide us in doing that.


Discrete : Step function

vs

Continuous : Sigmoid Function

probability




## Softmax

### Multi-Class Classification and Softmax

What if we have more than 2 classes ?

### The Softmax Function
We'll learn about the softmax function, which is the equivalent of the sigmoid activation function, but when the problem has 3 or more classes.

How do we turn these score into probabilities?

The First thing we need to satisfy with probabilities is they need to add to one.

The second thing we need to satisfy is higher score should have higher probabilities.

#### Softmax Function

Linear function

scores : z1,...zn

The probability that the object is in class i is going to be e to the power of the zi divided by the sum of e to the power of z1 plus all the way to e to the power zn. 



### Quiz: Coding Softmax
And now, your time to shine! Let's code the formula for the Softmax function in Python.

    import numpy as np

    # Write a function that takes as input a list of numbers, and returns
    # the list of values given by the softmax function.
    def softmax(L):
        expL=np.exp(L)
        sumexp=sum(expL)
        result =[]
        for i in expL :
            result.append(i*1.0/sumexp)
        return result



## One-Hot Encoding 

Sometimes input data is not number.

For two classes, we can just make one variable , separate with 1 and 0

But if there are more than 2 classes, you should not use 0,1,2 , because it would assume dependencies between them.

So we come up with one variable for each of the classes



## Maximum Likelihood

### Maximum Likelihood
Probability will be one of our best friends as we go through Deep Learning. In this lesson, we'll see how we can use probability to evaluate (and improve!) our models.

The best model would more likely be the one that gives the higher probabilities to the events that happend to us whether it's acceptance or rejection.

What we do is we pick the model that gives the existing labels the highest probability. Thus by maximizing the probability we can pick the best possible model



## Maximizing Probabilities 

Maximizing the probability and minimizing the error function.


products are very sensitive when there are thousands of data or close to 0.

So we will do sum.

we need to find a function that will help us turn products into sums.





## Cross-Entropy 1

We'll be taking the natural logarithm which is base e.

The logarithm of a number between 0 and 1 is always a negative number since the logarithm of one is zero

So the negative of the logarithm of the probabilities will become positive numbers.

So we will take the negative of the logarithm of the probabilities.

Good model will give us a low cross entropy 

Bad model will give us a high cross entropy

__Goal : Minimize the cross Entropy __



## Cross-Entropy 2

### Cross-Entropy
So we're getting somewhere, there's definitely a connection between probabilities and error functions, and it's called Cross-Entropy. This concept is tremendously popular in many fields, including Machine Learning. Let's dive more into the formula, and actually code it!



### Cross Entropy Formula

-∑ y_i\*ln(p_i) +(1-y_i)\*ln(1-p_i)


### Quiz: Coding Cross-entropy


    import numpy as np

    # Write a function that takes as input two lists Y, P,
    # and returns the float corresponding to their cross-entropy.
    def cross_entropy(Y, P):
        cross_ent=0
        for i in range(len(Y)) :
            cross_ent -= Y[i]*np.log(P[i])+(1-Y[i])*np.log(1-P[i])
        return cross_ent





## Multi-Class Cross Entropy


-∑∑ y_ij \* ln(p_ij)




## Logistic Regression

Now, we're finally ready for one of the most popular and useful algorithms in Machine Learning, and the building block of all that constitutes Deep Learning. The Logistic Regression Algorithm. And it basically goes like this:

* Take your data
* Pick a random model
* Calculate the error
* Minimize the error, and obtain a better model


### Calculating the Error Function
Let's dive into the details. The next video will show you how to calculate an error function.

Error = - (1-y) ln(i-y_hat)- y ln(y_hat)

Error Function = - 1/m ∑( (1-y_i) ln(1-y_hat)+ y_i ln(y_hat))

E(W,b) =- 1/m ∑( (1-y_i)ln(1-σ(Wx(i)+b)) + y_i ln(σ(Wx(i)+b))

__Goal : Minimize Error Function__


### Minimizing the error function

Start with random weight

Use gradient descent

Find new w', b' which makes small error function.





## Gradient Descent
In this lesson, we'll learn the principles and the math behind the gradient descent algorithm.

 
Inputs of the functions are W1 , W2 and error function is givene by E.

The gradient of E is given by the vector sum of the partial derivatives of E with respect to W1, W2. 

This gradient tell us the direction to move that increase the error fuction the most.

Thus, If we take the negaive of the gradient, this will tell us how decrease the eroor function the most. 

Take the negative of the gradient of the error function.


### Gradient Calculation
In the last few videos, we learned that in order to minimize the error function, we need to take some derivatives. So let's get our hands dirty and actually compute the derivative of the error function. The first thing to notice is that the sigmoid function has a really nice derivative. Namely,


σ'(x)=σ(x)(1−σ(x))

The error formula is:
 
E =- 1/m ∑( (1-y_i) ln(1-y_hat)+ y_i ln(y_hat))

So, a small gradient means we'll change our coordinates by a little bit, and a large gradient means we'll change our coordinates by a lot.


### radient Descent Step
Therefore, since the gradient descent step simply consists in subtracting a multiple of the gradient of the error function at every point, then this updates the weights 



## Logistic Regression Algorithm

1. Start with random weights : 
  w_1, ..., w_n , b

2. For every point (x_1,...,x_n) :

  2.1 For i=1...n

   2.1.1 Update w_i' <- w_i - alpha partial dev E of w_i (which is 9y_hat -y)x_i )
   2.1.2 Update b' <- b- alpha partial dev E of w_i (which is y_hat - y )

3. Reatpeat until error is small. 



##  Pre-Notebook: Gradient Descent


### Implementing Gradient Descent
In the following notebook, you'll be able to implement the gradient descent algorithm on the following sample dataset with two classes.


### Workspace
To open this notebook, you have two options:

> * Go to the next page in the classroom (recommended)
>
> * Clone the repo from [Github](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-neural-networks/gradient-descent) and open the notebook GradientDescent.ipynb in the intro-neural-networks > gradient-descent folder. You can either download the repository via the command line with git clone https://github.com/udacity/deep-learning-v2-pytorch.git .


### Instructions

In this notebook, you'll be implementing the functions that build the gradient descent algorithm, namely:

* sigmoid: The sigmoid activation function.
* output_formula: The formula for the prediction.
* error_formula: The formula for the error at a point.
* update_weights: The function that updates the parameters with one gradient descent step.

When you implement them, run the train function and this will graph the several of the lines that are drawn in successive gradient descent steps. It will also graph the error function, and you can see it decreasing as the number of epochs grows.

This is a self-assessed lab. If you need any help or want to check your answers, feel free to check out the solutions notebook in the same folder, or by clicking [here](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-neural-networks/gradient-descent/GradientDescentSolutions.ipynb).



## Perceptron vs Gradient Descent 

Gradient Descent Algorithm : 

Change w_i to w_i _ a(y-y_hat)x_i

Perceptron Algorithm :

If x is miscalssified : 

Change w_i to w_i + ax_i if positive / w_i- ax_i if negative

If correctly classified : y-y_hat =0

If missclassified : y-y_hat=1 if positive / y-y_Hat=-1 if negtive 


## Continuous Perceptrons 

We label the edges by the weights and the notde by the bias.

So what the perceptron does  is that takes points(inputs ) to plots it in the graph and then it returns a probability .



## Non-linear Data

Now we've been dealing a lot with data sets that can be separated by a line.

But as you can image the real world is much more complex than that.


## Non-Linear Models 

Find the curve that can separate data .

We're going to create a probability funciont where the points inthe blue region are more likely to be blue and the points in the red region are more likely to be red. 




## Neural Network Architecture

Ok, so we're ready to put these building blocks together, and build great Neural Networks! (Or Multi-Layer Perceptrons, however you prefer to call them.)

This first two videos will show us how to combine two perceptrons into a third, more complicated one.


We're going to combine two linear models into a nonlinear models as follows.

Calculate the probability for one of them, the probability for the other, then add them and then we apply the sigmoid function. 

What if we wanted to concentrate more on one model than the other ?

We can add weights even bias .



We can take a linear combination of two models and that gives us non-linear model.

This looks a lot like perceptron where we can take a value times a constant plus another value times a constant plust a bias and get a new value. 

This is actually the building block of Neural Networks.


### Multiple layers

Now, not all neural networks look like the one above. They can be way more complicated! In particular, we can do the following things:

* Add more nodes to the input, hidden, and output layers.
* Add more layers.

Neural networks have a certain special architecture with layers.
* First layer is the intput layer, which cotains the inputs.
* The next layer is the hidden layer, which is a set of linear models created with the first input layer.
* The Final layer is the output layer, where the linear models get combined to obtain a nonlinear model. 

What if our output layer has more nodes? Then we just have more outputs. In that case, we just have a mulitclass classification model.

What if we  have more layers?  Then we have what's called a deep neural network.


### Multi-Class Classification
And here we elaborate a bit more into what can be done if our neural network needs to model data with more than one output.

We will add more nodes in the output layer and each one of the node will give us the probability of each class.

Now, wetake the score and apply the SoftMax function to obtain well defined probabilites.




## Feedforward
Feedforward is the process neural networks use to turn the input into an output. Let's study it more carefully, before we dive into how to train the networks.

We need to how to train them. 

Training them means what parameters should they have on the edges in order to model our data well. 



###  Error Function
Just as before, neural networks will produce an error function, which at the end, is what we'll be minimizing. The following video shows the error function for a neural network.



## Backpropagation

Now, we're ready to get our hands into training a neural network. For this, we'll use the method known as backpropagation. In a nutshell, backpropagation will consist of:

* Doing a feedforward operation.
* Comparing the output of the model with the desired output.
* Calculating the error.
* Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.
* Use this to update the weights, and get a better model.
* Continue this until we have a model that is good.


### Backpropagation Math
And the next few videos will go deeper into the math. Feel free to tune out, since this part gets handled by Keras pretty well. If you'd like to go start training networks right away, go to the next section. But if you enjoy calculating lots of derivatives, let's dive in!


### Chain Rule
We'll need to recall the chain rule to help us calculate derivatives.

### Calculation of the derivative of the sigmoid function

Recall that the sigmoid function has a beautiful derivative, which we can see in the following calculation. This will make our backpropagation step much cleaner.


## Pre-Notebook: Analyzing Student Data

### Notebook: Analyzing Student Data

Now, we're ready to put neural networks in practice. We'll analyze a dataset of student admissions at UCLA.

To open this notebook, you have two options:

> * Go to the next page in the classroom (recommended).
> * Clone the repo from Github and open the notebook StudentAdmissions.ipynb in the intro-neural-networks > student_admissions folder. You can either download the repository with git clone https://github.com/udacity/deep-learning-v2-pytorch.git, or download it as an archive file from this link.


### Instructions

In this notebook, you'll be implementing some of the steps in the training of the neural network, namely:

* One-hot encoding the data
* Scaling the data
* Writing the backpropagation step

This is a self-assessed lab. If you need any help or want to check your answers, feel free to check out the solutions notebook in the same folder, or by clicking [here](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-neural-networks/student-admissions/StudentAdmissionsSolutions.ipynb).


















