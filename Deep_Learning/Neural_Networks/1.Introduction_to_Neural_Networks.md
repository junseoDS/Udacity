# Introduction to Neural Networks


## Instructor

Hello and welcome to Introduction to Neural Networks, given by Luis!

Luis was formerly a Machine Learning Engineer at Google. He holds a PhD in mathematics from the University of Michigan, and a Postdoctoral Fellowship at the University of Quebec at Montreal.



## Introduction 

### "What is Deep Learning "

### "What is it used for ? "
> Pretty much everywhere

### Neural Networks
> Neural networks vaguely mimic the process of how the brain operates, with neurons that fire bits of information.



## Classification Problems 1


### Classification Problems
We'll start by defining what we mean by classification problems, and applying it to a simple example.

##### Acceptance at a University

two information : 
* Test
* Grades

__Student 1__
Test : 9/10

Grades : 8/10

Accepted

__Student 2__

Test : 3/10

Grades : 4/10

Rejected

__Student 3__

Test :7/10

Grades : 6/10

???

## Classification Problems 2

"How do we find this line(MODEL) ?"


## Linear Boundaries 

x_1 : Test

x_2 : Grades 

Boundary : A Line

2x_1 + x_2 -18 =0

Score = 2\*Test + Grades - 18

Prediction : 

Score > 0 : Accept

Score < 0 : Reject


In more general case, our boundary will be an equation of the following :

w_1x_1 + w_2x_2 + b = 0 

Wx +b =0

W=(w_1,w_2)

x=(x_1,x_2)

y= label : 0 or 1

Prediction:

y_hat = 1 if Wx +b >= 0

"      = 0 if Wx +b <0



## Higher Dimensions 

### 3 dimension

Boundary : A Plane

w_1x_1 + w_2x_2 + w_3x_3 +b =0

Wx +b =0

Prediciotn :

y_hat = 1 if Wx + b >= 0

"     = 0 if Wx + b < 0



### n-dimensional space 

x_1, x_2, ... x_3

Boundary : n-1 dimensional hyperplane

w_1x_1 + w_2x_2 + ... w_nx_n + b = 0

Wx + b = 0

Prediction :

y_hat = 1 if Wx +b >= 0

  "   = 0 of Wx +b < 0 



## Perceptrons

Wx + b = ∑WiXi + b

### Step Function 

y = 1 if x >= 0

y = 0 if x < 0


## Why "Neural Networks ? "

the reason why they're called neural network is because perceptrons kind of look like neurons in the brain. 




## Perceptrons as Logical Operators

In this lesson, we'll see one of the many great applications of perceptrons. As logical operators! You'll have the chance to create the perceptrons for the most common of these, the AND, OR, and NOT operators. And then, we'll see what to do about the elusive XOR operator. Let's dive in!

### AND Perceptron 

The inputs can be true or false, but the output is only true if both of the inputs are true. 

### Or Perceoptron

OR operatior which returns true if any of its two inputs is true . 


### What are the weights and bias for the AND perceptron?
Set the weights (weight1, weight2) and bias (bias) to values that will correctly determine the AND operation as shown above.
More than one set of values will work!

    import pandas as pd

    # TODO: Set weight1, weight2, and bias
    weight1 = 0.0
    weight2 = 0.0
    bias = 0.0


    # DON'T CHANGE ANYTHING BELOW
    # Inputs and outputs
    test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
    correct_outputs = [False, False, False, True]
    outputs = []

    # Generate and check output
    for test_input, correct_output in zip(test_inputs, correct_outputs):
        linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
        output = int(linear_combination >= 0)
        is_correct_string = 'Yes' if output == correct_output else 'No'
        outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])

    # Print output
    num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
    output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])
    if not num_wrong:
        print('Nice!  You got it all correct.\n')
    else:
        print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
    print(output_frame.to_string(index=False))



### OR Perceptron

he OR perceptron is very similar to an AND perceptron. The OR perceptron has the same line as the AND perceptron, except the line is shifted down. What can you do to the weights and/or bias to achieve this? Use the following AND perceptron to create an OR Perceptron.


### NOT Perceptron
Unlike the other perceptrons we looked at, the NOT operation only cares about one input. The operation returns a 0 if the input is 1 and a 1 if it's a 0. The other inputs to the perceptron are ignored.

In this quiz, you'll set the weights (weight1, weight2) and bias bias to the values that calculate the NOT operation on the second input and ignores the first input.



    import pandas as pd

    # TODO: Set weight1, weight2, and bias
    weight1 = 0.0
    weight2 = 0.0
    bias = 0.0


    # DON'T CHANGE ANYTHING BELOW
    # Inputs and outputs
    test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
    correct_outputs = [True, False, True, False]
    outputs = []

    # Generate and check output
    for test_input, correct_output in zip(test_inputs, correct_outputs):
        linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
        output = int(linear_combination >= 0)
        is_correct_string = 'Yes' if output == correct_output else 'No'
        outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])

    # Print output
    num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
    output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])
    if not num_wrong:
        print('Nice!  You got it all correct.\n')
    else:
        print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
    print(output_frame.to_string(index=False))
    
    
    
### XOR Perceptron

### Quiz: Build an XOR Multi-Layer Perceptron
Now, let's build a multi-layer perceptron from the AND, NOT, and OR perceptrons to create XOR logic!

The neural network below contains 3 perceptrons, A, B, and C. The last one (AND) has been given for you. The input to the neural network is from the first node. The output comes out of the last node.

The multi-layer perceptron below calculates XOR. Each perceptron is a logic operation of AND, OR, and NOT. However, the perceptrons A, B, and C don't indicate their operation. In the following quiz, set the correct operations for the perceptrons to calculate XOR.


A : AND B : OR C : NOT


## Perceptron Trick 

### Perceptron Trick
In the last section you used your logic and your mathematical knowledge to create perceptrons for some of the most common logical operators. In real life, though, we can't be building these perceptrons ourselves. The idea is that we give them the result, and they build themselves. For this, here's a pretty neat trick that will help us.

#### Split Data 

> Does the misclassified point want the line to be closer or farther?
>
> Closer

### Time for some math!
Now that we've learned that the points that are misclassified, want the line to move closer to them, let's do some math. The following video shows a mathematical trick that modifies the equation of the line, so that it comes closer to a particular point.


## Perceptron Algorithm

### Perceptron Algorithm
And now, with the perceptron trick in our hands, we can fully develop the perceptron algorithm! The following video will show you the pseudocode, and in the quiz below, you'll have the chance to code it in Python.

1. Start with random weight : w1,...wn,b

2. For every misclassified point(x1,..xn) :

  2.1 If prefiction =0 :
  
.    -For i = 1...n
    
.     -Change wi = wi + axi

  2.2 If prediction =1 :
    
.    - For i = 1...n 
    
.      - Change wi = wi-axi
      
.    - Change b to b-a


### Coding the Perceptron Algorithm
Time to code! In this quiz, you'll have the chance to implement the perceptron algorithm to separate the following data (given in the file data.csv).

Recall that the perceptron step works as follows. For a point with coordinates (p,q)(p,q), label yy, and prediction given by the equation \hat{y} = step(w_1x_1 + w_2x_2 + b) 

* If the point is correctly classified, do nothing.
* If the point is classified positive, but it has a negative label, subtract \alpha p, \alpha q,αp,αq, and \alphaα from w_1, w_2, and b respectively.
If the point is classified negative, but it has a positive label, add \alpha p, \alpha q,αp,αq, and \alphaα to w_1, w_2, and b respectively.

Then click on test run to graph the solution that the perceptron algorithm gives you. It'll actually draw a set of dotted lines, that show how the algorithm approaches to the best solution, given by the black solid line.

Feel free to play with the parameters of the algorithm (number of epochs, learning rate, and even the randomizing of the initial parameters) to see how your initial conditions can affect the solution!



    import numpy as np
    # Setting the random seed, feel free to change it and see different solutions.
    np.random.seed(42)

    def stepFunction(t):
        if t >= 0:
            return 1
        return 0

    def prediction(X, W, b):
        return stepFunction((np.matmul(X,W)+b)[0])

    # TODO: Fill in the code below to implement the perceptron trick.
    # The function should receive as inputs the data X, the labels y,
    # the weights W (as an array), and the bias b,
    # update the weights and bias W, b, according to the perceptron algorithm,
    # and return W and b.
    def perceptronStep(X, y, W, b, learn_rate = 0.01):
        # Fill in code
        for i in range(len(X)) :
            y_hat= prediction(X[i],W,b)
            if y[i]-y_hat==1:
                W[0] += W[0]*learn_rate
                W[1] += W[1]*learn_rate
                b += b*learn_rate
            elif y[i]-y_hat==-1:
                W[0] -= W[0]*learn_rate
                W[1] -= W[1]*learn_rate
                b -= b*learn_rate
        return W, b

    # This function runs the perceptron algorithm repeatedly on the dataset,
    # and returns a few of the boundary lines obtained in the iterations,
    # for plotting purposes.
    # Feel free to play with the learning rate and the num_epochs,
    # and see your results plotted below.
    def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):
        x_min, x_max = min(X.T[0]), max(X.T[0])
        y_min, y_max = min(X.T[1]), max(X.T[1])
        W = np.array(np.random.rand(2,1))
        b = np.random.rand(1)[0] + x_max
        # These are the solution lines that get plotted below.
        boundary_lines = []
        for i in range(num_epochs):
            # In each epoch, we apply the perceptron step.
            W, b = perceptronStep(X, y, W, b, learn_rate)
            boundary_lines.append((-W[0]/W[1], -b/W[1]))
        return boundary_lines




## Non-Linear Regions

Sometimes the data cannot be separated by just a line.

So what is the next thing after a line?

Maybe circle, two lines, or a curve 

__We need to redefine our perceptron algorithm for a line in a way that it'll generalize to other types of curves.__



## Error Functions 

Error function is simply something that tells us how far we are from the solution. 

= Distance 



## Log-loss Error Function

The error is what's telling us how badly we're doing at the moment and gow far we from an ideal solution.

And if we constantly take steps to decrease the error , then we eventually solvethe problem.

### Discrete vs Continuous

Error function can not be discrete, it should be continuous. 




## Descrete vs Continuous

### Discrete vs Continuous Predictions
We learned that continuous error functions are better than discrete error functions, when it comes to optimizing. For this, we need to switch from discrete to continuous predictions. The next two videos will guide us in doing that.


Discrete : Step function

vs

Continuous : Sigmoid Function

probability






