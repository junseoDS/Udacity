# Embedding & Word2Vec


## Word Embeddings


### Word Embeddings

The collective term for models that learned to map a set of words or phrases in vocabulary to vectors of numerical values.  These vectors are called embeddings 

In general this technique is used to reduce the dimensionality of text data. But these embedding models can also learn some interesting traits about words in a vocabulary.

### Word2Vec model
Learns to map words to embedding that contain semantic meaning. Embeddings can learn the relationship between verb tenses and gendered words. Risk is biased or incorrect mappings from source text. 



## Embedding Weight Matrix/Lookup Table

Word embedding is all abouti improving the ability of networks to learn from texted data. Embeddings can greatly improve the ability of networks to learn text data, by representing that data as lower-dimensional vectors. 

__One-hot encoded words__

Thousands of values long  and only one of them is set to one, and all the others are set to zero. 
Then, you pass this long vector as input to some hidden layer in the network. The output of this hidden layer is calculated by multiplying that input vector by some matrix of learned weights. The result is huge matrix of values. Most of which are zero, because of the initial one-hot vector. So all these computaing resources are used on values that do not hold any information, and this tis computationally inefficient.  To solve this problem, we can use embeddings, 

__Embeddings__

Embeddings provide a shortcut for doing this matrix multiplication. To learn word embeddings, we use a fully-connected linear layer which we will call the embedding layer, and its weights are the embedding weights. These weights will be values that are learned during training this embedding model, and they make up a useful weight matrix. With this matrix, we can __skip the big multiplication step from before, by instead grabbing the values for the output of our hidden layer directly from a row in our weight matrix.__ We can do this because the multiplication of a one-hot encoded vector with a weight matrix, returns only the row of the matrix that corresponds to index of the one or the on input unit.
So we use the embedding weight matrix as a lookup table. Instead of representing words as one-hot vectors, we can encode each word as a unique integer. This process is called an embedding lookup, and  the number of hidden units is the embedding dimension.

So, the embedding lookup table is just a weight matrix, and the embedding layer is just a hidden layer.
It's important to know that lookup talbe holds weights that are learned during training just like any weight matrix. 


## Word2Vec Notebook

Skip_Grams_Exercise.ipynb

Before I go through this code, I'd recommend reading a couple papers on Word2Vec that have inspired the code that we'll be going through. You can find links to these papers in the Supporting Materials section, below!

지원 자료
 [Word2Vec Mikolov](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bc56d28_word2vec-mikolov/word2vec-mikolov.pdf)
 [Distributed Representations, Mikolov 2](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bc56da8_distributed-representations-mikolov2/distributed-representations-mikolov2.pdf)



Subsampling equation
P(w_i) = 1 - sqrt(t/f(w_i))

For the following quiz question, consider the following data points:

We have a text with 1 million words in it
The word "learn" appears 700 times in this text



## Negative Sampling

Negative_Sampling_Exercise.ipynb





Read more about writing [custom extensions in PyTorch](https://pytorch.org/docs/master/notes/extending.html).

















