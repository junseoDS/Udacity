# Embedding & Word2Vec


## Word Embeddings


### Word Embeddings

The collective term for models that learned to map a set of words or phrases in vocabulary to vectors of numerical values.  These vectors are called embeddings 

In general this technique is used to reduce the dimensionality of text data. But these embedding models can also learn some interesting traits about words in a vocabulary.

### Word2Vec model
Learns to map words to embedding that contain semantic meaning. Embeddings can learn the relationship between verb tenses and gendered words. Risk is biased or incorrect mappings from source text. 



## Embedding Weight Matrix/Lookup Table





























