# Implementation of RNN & LSTM


## Implementing RNNs

In this lesson, Matt and I will going over some implementations of these networks. Becauase RNNs have a kind of built in memory, they're really useful for tasks that are time or sequence dependent.

The challenges in designing and implementing any kind of RNN are really two-fold.

* Pre-process sequential data
  such as a series of sentences, and convert it into numerical data that can be understood by a neural network?
  
* Represent memory in code
  



## Time-Series Prediction


### Code Walkthrough & Repository
The below video is a walkthrough of code that you can find in our public Github repository, if you navigate to recurrent-neural-networks > time-series and the Simple_RNN.ipynb notebook. Feel free to go through this code on your own, locally.

This example is meant to give you an idea of how PyTorch represents RNNs and how you might represent memory in code. Later, you'll be given more complex exercise and solution notebooks, in-classroom.
  

Simple_RNN.ipynb



Recurrent Layers
Here is the documentation for the main types of [recurrent layers in PyTorch](https://pytorch.org/docs/stable/nn.html#recurrent-layers). Take a look and read about the three main types: RNN, LSTM, and GRU.

Hidden State Dimensions




## Character-wise RNNs 



## Sequence Batching

One of the most difficult parts of building networks is getting the batches right. 

By taking a sequence and splitting it into multiple shorter sequences, we can take advantage of matrix operations to make training more efficient. 

The batch size corresponds to the number of sequences we are using. 
Along with batch size, we choose the length of the sequences we feed to the network. 



## Implementing a Char-RNN

Character_Level_RNN_Exercise.ipynb



## Define the model

Character_Level_RNN_Exercise.ipynb

#### Contiguous variables

If you are stacking up multiple LSTM outputs, it may be necessary to use .contiguous() to reshape the output. The notebook and Github repo code has been updated to include this use case in the forward function of the model:

    # stack up LSTM outputs
    out = out.contiguous().view(-1, self.n_hidden)




#### Examples of RNNs

Take a look at one of my favorite examples of RNNs making predictions based on some user-generated input dat: [the sketch-rnn by Magenta](https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html). This RNN takes as input a starting sketch, drawn by you, and then tries to complete your sketch using a particular model. For example, it can learn to complete a sketch of a pineapple or the mona lisa!

