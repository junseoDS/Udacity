# Implementation of RNN & LSTM


## Implementing RNNs

In this lesson, Matt and I will going over some implementations of these networks. Becauase RNNs have a kind of built in memory, they're really useful for tasks that are time or sequence dependent.

The challenges in designing and implementing any kind of RNN are really two-fold.

* Pre-process sequential data
  such as a series of sentences, and convert it into numerical data that can be understood by a neural network?
  
* Represent memory in code
  



## Time-Series Prediction


### Code Walkthrough & Repository
The below video is a walkthrough of code that you can find in our public Github repository, if you navigate to recurrent-neural-networks > time-series and the Simple_RNN.ipynb notebook. Feel free to go through this code on your own, locally.

This example is meant to give you an idea of how PyTorch represents RNNs and how you might represent memory in code. Later, you'll be given more complex exercise and solution notebooks, in-classroom.
  





















