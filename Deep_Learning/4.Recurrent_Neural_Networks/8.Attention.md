# Attention

## Introduction to Attention

Attention started out in the field of computer vision as an attempt to mimic human perception.


"One important property of human perception is that one does not tend to process a whole scene in its entirety at once. Instead humans focus attention selectively on parts of the visual space to acquire information when and where it is needed, and combine information from different fixations over time to build up an internal representation of the scene, guiding future eye movements and decision making."



## Encoders and Decoders

### Sequence to Sequence Models
Before we jump into learning about attention models, let's recap what you've learned about sequence to sequence models. We know that RNNs excel at using and generating sequential data, and sequence to sequence models can be used in a variety of applications!




### Encoders and Decoders
The encoder and decoder do not have to be RNNs; they can be CNNs too!

In the example above, an LSTM is used to generate a sequence of words; LSTMs "remember" by keeping track of the input words that they see and their own hidden state.

In computer vision, we can use this kind of encoder-decoder model to generate words or captions for an input image or even to generate an image from a sequence of input words. We'll focus on the first case: generating captions for images, and you'll learn more about caption generation in the next lesson. For now know that we can input an image into a CNN (encoder) and generate a descriptive caption for that image using an LSTM (decoder).





## Sequence to Sequence Recap

A sequence to sequence model takes in an input that is a sequence of items, and then it produces another sequence of items as an output

In a machine translation application, the input sequence is a series of words in one language, and the output is the translation in another language.

A sequence to sequence model usually consists of an encoder and decoder, and it works by the encoder first processing all of the inputs turning the inputs into single representation, typically a single vector. This is called __context vector__ and it contains whatever information the encoder was able to capture from the input sequence. This vector is then sent to the decoder which uses it to formulate an output sequence.


## Encoding --Attention Overview

A Sequence to Sequence Model with Attention works in the following way :

* First, the encoder processes the input sequence one word at a time, producing a hidden state and using that hidden state and the next step. 

* Next, the model passes a context vector to the decoder but unlike the context vector in the model without attention, this one is not just the final hidden state, __it's all of the hidden states.__ This gives us the benefit of having the flexibility in the context size. __So longer sequences can have longer context vectors that better capture the information from the input sequence.__

* One additional point that's important for the intuition of attention, is that each hidden state is sort of associated the most with the part of the input sequence that preceded how that word was generated.
  * So the first hidden state was outputted after processing the first word, so it captures the essence of the first word the most. 



## Decoding -- Attention Overview

How the attention decoder works at a high level.

At every timesteps, an attention decoder pays attention to the appropriate part of the input sequence using the context factor. 

How does the attention decoder know which of the parts of the input sequence to focus on at each step?
* That process is learned during the training phase. It can learn some sophisticated behavior. 
 




## Attention Encoder

The Encoder is RNN. When creating a RNN, we have to declare the number of hidden units in the RNN cell. This applies whether we have a vanilla RNN or an LSTM or GRU cell. 




## Attention Decoder

In models without attention, we'd only feed the last context vector to the decoder RNN, in addition to the embedding of the end token, and it will begin to generate an element of the output sequence at each time-step. 

The case is different in an attention decoder.
An attention decoder has the ability to look at the inputted words, and decoders own hidden state, and then it would do the following. It would use a scoring function to score each hidden state in the context matrix.  After scoring each context vector would end up with a certain score and if we feed these scores into __a softmax function__, we end up with scores that are all positive, that are all between 0 and 1, and that all sum up to 1. These values are how much each vector will be expressed in the attention vector that the decoder will look at before producing an output. Simply multiplying each vector by its softmax score and then, summing up these vectors produces an attention contexts vectors, this is a basic weighted sum operation. The context vector is an important milestone in this process, but it's not end goal.  

The decoder has now looked at the input word and at the attention context vector, which focused its attention on the appropriate place in the input sequence. So it produces a hidden state and it produces the first word in the output sequence. 
In the next time step, the RNN takes its previous output as an input, and it generates its own contwext vector for that time step, and that produces new hidden state for the decoder, and new word in the ouput sequence, and this goes on until we've completed our output sequence.  


## Bahdanau and Luong Attention

### Additive and Multiplicative Attention

#### Bahdanau Attention (Additive)

[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)

#### Luong Attention (Multiplicative)
[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)


## Multiplicative Attention

An attention scoring function tends to be a function that takes in the hidden state of the decoder and the set of hidden states of the encoder.



## Computer Vision Applications


Super interesting computer vision applications using attention:

[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf) [pdf]

[Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](https://arxiv.org/pdf/1707.07998.pdf) [pdf]

[Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S19-04.pdf)[pdf]

[Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos](https://arxiv.org/pdf/1507.05738.pdf) [pdf]

[Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge](https://arxiv.org/pdf/1708.02711.pdf) [pdf]

[Visual Question Answering: A Survey of Methods and Datasets](https://arxiv.org/pdf/1607.05910.pdf) [pdf]




## Other Attention Methods

Paper: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

Talk: [Attention is all you need attentional neural network models – Łukasz Kaiser](https://www.youtube.com/watch?v=rBCqOTEfxvg)



## Notebook : Attention Basics

Attention_Basics.ipynb





































