# Attention

## Introduction to Attention

Attention started out in the field of computer vision as an attempt to mimic human perception.


"One important property of human perception is that one does not tend to process a whole scene in its entirety at once. Instead humans focus attention selectively on parts of the visual space to acquire information when and where it is needed, and combine information from different fixations over time to build up an internal representation of the scene, guiding future eye movements and decision making."



## Encoders and Decoders

### Sequence to Sequence Models
Before we jump into learning about attention models, let's recap what you've learned about sequence to sequence models. We know that RNNs excel at using and generating sequential data, and sequence to sequence models can be used in a variety of applications!




### Encoders and Decoders
The encoder and decoder do not have to be RNNs; they can be CNNs too!

In the example above, an LSTM is used to generate a sequence of words; LSTMs "remember" by keeping track of the input words that they see and their own hidden state.

In computer vision, we can use this kind of encoder-decoder model to generate words or captions for an input image or even to generate an image from a sequence of input words. We'll focus on the first case: generating captions for images, and you'll learn more about caption generation in the next lesson. For now know that we can input an image into a CNN (encoder) and generate a descriptive caption for that image using an LSTM (decoder).





## Sequence to Sequence Recap

A sequence to sequence model takes in an input that is a sequence of items, and then it produces another sequence of items as an output

In a machine translation application, the input sequence is a series of words in one language, and the output is the translation in another language.

A sequence to sequence model usually consists of an encoder and decoder, and it works by the encoder first processing all of the inputs turning the inputs into single representation, typically a single vector. This is called __context vector__ and it contains whatever information the encoder was able to capture from the input sequence. This vector is then sent to the decoder which uses it to formulate an output sequence.


## Encoding --Attention Overview

A Sequence to Sequence Model with Attention works in the following way :

* First, the encoder processes the input sequence one word at a time, producing a hidden state and using that hidden state and the next step. 

* Next, the model passes a context vector to the decoder but unlike the context vector in the model without attention, this one is not just the final hidden state, __it's all of the hidden states.__ This gives us the benefit of having the flexibility in the context size. __So longer sequences can have longer context vectors that better capture the information from the input sequence.__

* One additional point that's important for the intuition of attention, is that each hidden state is sort of associated the most with the part of the input sequence that preceded how that word was generated.
  * So the first hidden state was outputted after processing the first word, so it captures the essence of the first word the most. 



## Decoding -- Attention Overview

How the attention decoder works at a high level.

At every timesteps, an attention decoder pays attention to the appropriate part of the input sequence using the context factor. 

How does the attention decoder know which of the parts of the input sequence to focus on at each step?
* That process is learned during the training phase. It can learn some sophisticated behavior. 
 




## Attention Encoder





