# Hyperparameters


## Introducing Jay


For this section, we're introducing a new Udacity instructor, Jay Alammar. Jay has done some great work in interactive explorations of neural networks, check out [his blog](http://jalammar.github.io/).

Jay will be reviewing some of the material you saw in the Deep Neural Networks section on hyperparameters, and he will also introduce the hyperparameters used in Recurrent Neural Networks.



## Introduction

In this lesson, we will be talking about a number of different __hyper parameters__ and __identyfying possible starting values__ and __intuitions__ for a number of hyper parameters that you may have already come across.  

__A Hyper Parameter__ is a variable that we need to set before applying a learning algorithm into a dataset. 

Generally speaking, we can break hyper parameters down into two categories :

* Optimizer hyperparameters
  Variable related more to the optimization and training process than to the model it self.
  * learning rate
  * minibatch size
  * number of training iterations or epochs.
  
* Model hyperparameters
  Variables that are more involved in the structure of the model.
  * number of layers and hidden units
  * model specific hyper parameters for architectures




## Learning Rate

The learning rate is the most important hyperparameters. 

Good starting point = 0.01

And ususal suspects of learning rates :
* 0.1
* 0.01
* 0.001
* 0.0001
* 0.00001
* 0.000001

depends on the behavior of the training error. 




[Exponential Decay](https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay) in TensorFlow.

#### Adaptive Learning Optimizers
* [AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)
* [AdagradOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer)




## Minibatch Size

Historically there had been debate on whether it's better to do __online (stochastic) training__ where you fit a single example of the dataset to the model during a traing step, do a forward pass, calculate error and backpropagate and set adjusted values for all your parameters.  Or if it was better to feed the entire dataset to the training step and calculate the gradient using the error genrated by looking at all the examples in the dataset. This is called __batch training__. 

A larger minibatch size allows computational boosts that utilizes matrix multiplication, in the training calculation. But that comes at the expense of needing more memory for the training process, more computational resources. 

In practice, small minibatch sizes have more noise in their error calculation, and this noise is often helpful in preventing the training process from stopping at local minuma on the error curve rather than the global minima that created the best model. 

Too small could be too slow, too large could be computationally taxing and could result in worse accuracy.

32 to 256 are good starting values.


[Systematic evaluation of CNN advances on the ImageNet](https://arxiv.org/abs/1606.02228) by Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas




## Number of Training Iterations / Epochs

The intuitive manual wat is to have the model train for as many epochs or iterations that it takes, as long as the validation error keeps decreasing. 

__Early stop__ to determine when to stop training a model. 
Early stopping roughly works by monitoring the validation error, and stopping the training when it stops decreasing. We must be a liitle flexible though in defining the stopping trigger.
Validation error will often move back and forth, even if it's on a downward trend. So instead of stopping the training the first time we see the validation error increase, we can instead stop the training if the validation error has not improved in the last 10 or 20 steps.  

The number of training iterations is a hyperparameter we can optimize automatically using a technique called early stopping (also "early termination").


### ValidationMonitor (Deprecated)

In tensorflow, we can use a [ValidationMonitor with tf.contrib.learn](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/monitors) to not only monitor the progress of training, but to also stop the training when certain conditions are met.

The following example from the ValidationMonitor documentation shows how to set it up. Note that the last three parameters indicate which metric we're optimizing.


    validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(
      test_set.data,
      test_set.target,
      every_n_steps=50,
      metrics=validation_metrics,
      early_stopping_metric="loss",
      early_stopping_metric_minimize=True,
      early_stopping_rounds=200)

The last parameter indicates to ValidationMonitor that it should stop the training process if the loss did not decrease in 200 steps (rounds) of training.

The validation_monitor is then passed to tf.contrib.learn's "fit" method which runs the training process:


    classifier = tf.contrib.learn.DNNClassifier(
      feature_columns=feature_columns,
      hidden_units=[10, 20, 10],
      n_classes=3,
      model_dir="/tmp/iris_model",
      config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1))

    classifier.fit(x=training_set.data,
               y=training_set.target,
               steps=2000,
               monitors=[validation_monitor])

#### SessionRunHook

More recent versions of TensorFlow deprecated monitors in favor of SessionRunHooks. [SessionRunHooks](https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook) are an evolving part of tf.train, and going forward appear to be the proper place where you'd implement early stopping.

At the time of writing, two pre-defined stopping monitors exist as a part of tf.train's [training hooks](https://www.tensorflow.org/api_guides/python/train#Training_Hooks):

* [StopAtStepHook](https://www.tensorflow.org/api_docs/python/tf/train/StopAtStepHook): A monitor to request the training stop after a certain number of steps
* [NanTensorHook](https://www.tensorflow.org/api_docs/python/tf/train/NanTensorHook): a monitor that monitor's loss and stops training if it encounters a NaN loss




## Number of Hidden Units / Layers

The main requirement here is to set a number of hidden units that is quote unquote large enough.
The number and architecture of the hidden units is the main measure for a model's learning capacity. 

You could also utilize __regularization techniques like dropout or L2 regularizaion__. 

Add more hidden units and track validation error. Keep adding hidden units until the validation starts getting worse. 

Another heurisitic involving the first hidden layer is that setting it to a number larger thatn the number of the inputs that has been observe to be beneficial in a number of tasks. 


"in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers)." ~ Andrej Karpathy in https://cs231n.github.io/neural-networks-1/

#### More on Capacity

A more detailed discussion on a model's capacity appears in the [Deep Learning book](http://www.deeplearningbook.org/contents/ml.html), chapter 5.2 (pages 110-120).


## RNN Hyperparameters

Two main choices we need to make when we want to build a recurrent neural network are :

* choosing a cell type :
  so a long short term memory (LSTM) cell or a vanilla RNN cell or a gated recurrent unit (GRU) cell, 

* how deep the model is :
  how many layers will we stack? 




### LSTM Vs GRU

"These results clearly indicate the advantages of the gating units over the more traditional recurrent units. Convergence is often faster, and the final solutions tend to be better. However, our results are not conclusive in comparing the LSTM and the GRU, which suggests that the choice of the type of gated recurrent unit may depend heavily on the dataset and corresponding task."

[Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555) by Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio

"The GRU outperformed the LSTM on all tasks with the exception of language modelling"

[An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf) by Rafal Jozefowicz, Wojciech Zaremba, Ilya Sutskever

"Our consistent finding is that depth of at least two is beneficial. However, between two and three layers our results are mixed. Additionally, the results are mixed between the LSTM and the GRU, but both significantly outperform the RNN."

[Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078) by Andrej Karpathy, Justin Johnson, Li Fei-Fei

"Which of these variants is best? Do the differences matter? [Greff, et al. (2015)](https://arxiv.org/pdf/1503.04069.pdf) do a nice comparison of popular variants, finding that theyâ€™re all about the same. [Jozefowicz, et al. (2015)](http://proceedings.mlr.press/v37/jozefowicz15.pdf) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks."

[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah

"In our [Neural Machine Translation] experiments, LSTM cells consistently outperformed GRU cells. Since the computational bottleneck in our architecture is the softmax operation we did not observe large difference in training speed between LSTM and GRU cells. Somewhat to our surprise, we found that the vanilla decoder is unable to learn nearly as well as the gated variant."

[Massive Exploration of Neural Machine Translation Architectures] by Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le




## Sources & References


If you want to learn more about hyperparameters, these are some great resources on the topic:

  * [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/abs/1206.5533) by Yoshua Bengio

  * [Deep Learning book - chapter 11.4: Selecting Hyperparameters](http://www.deeplearningbook.org/contents/guidelines.html) by Ian Goodfellow, Yoshua Bengio, Aaron Courville

  * [Neural Networks and Deep Learning book - Chapter 3: How to choose a neural network's hyper-parameters?](http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters) by Michael Nielsen

  * [Efficient BackProp (pdf)](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) by Yann LeCun


More specialized sources:

  * [How to Generate a Good Word Embedding?](https://arxiv.org/abs/1507.05523) by Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao
  * [Systematic evaluation of CNN advances on the ImageNet](https://arxiv.org/abs/1606.02228) by Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas
  * [Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078) by Andrej Karpathy, Justin Johnson, Li Fei-Fei














