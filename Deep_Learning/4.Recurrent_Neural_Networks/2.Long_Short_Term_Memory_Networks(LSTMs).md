# Long Short-Term Memory Networks (LSTMs)


## Intro to LSTM

Hi! It's Luis again!

Now that you've gone through the Recurrent Neural Network lesson, I'll be teaching you what an LSTM is. This stands for Long Short Term Memory Networks, and are quite useful when our neural network needs to switch between remembering recent things, and things from long time ago. But first, I want to give you some great references to study this further. There are many posts out there about LSTMs, here are a few of my favorites:

* [Chris Olah's LSTM post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [Edwin Chen's LSTM post](http://blog.echen.me/2017/05/30/exploring-lstms/)
* [Andrej Karpathy's lecture on RNNs and LSTMs from CS231n](https://www.youtube.com/watch?v=iX5V1WpxxkY)

So, let's dig in!



## RNN vs LSTM

### RNN
drawback : information coming in gets repeatedly squished by sigmoid functions and even worse than that, training a network using backpropagation all the way back, will lead to problem such as the vanishing gradient problem etc.  The memory that is stored is normally short term memory. 

Memory comes in and merges with a current event and the output comes out as a prediction of what the input is. And also, as part of the input for the next iteration of the neural network. 

### LSTM

It keeps track not just of memory but of long term memory, which comes in and comes out. And also, short term memory, which also comes in and comes out. And in every stage, the long and short term memory in the event get merged. And from there, we get a new long term memory, short term memory and prediction. 



## Basics of LSTM

Long Term Memory
Short Term Memory
Event

LSTM works as follows : 
the three pieces of information go inside the node and then some math happens and then the new pieces of information get updated and come out. There is a long term memory, a short term memory and the prediction of the event. 

More specifically the architecture of LSTM contains a few gates : 

* forget gate
* learn gate
* remember gate
* use gate 

The long term memory goes to the forget gate where it forgets everything that it doesn't consider useful.  
The short term memory and the event are joined together in the learn gate, containing the information that we've recently  learned and it  removes any unnecessary information. 
Now the long term memory that we haven't forgotten yet plus the new  information that we've learned get joined together in the remember gate. This gate puts these two together and since it's called remember gate, what it does is it outputs and updated long term memory. So this is what we'll remember for the future. 
And finally, the use gates is the one that decides what information we use from what we previously know plus what we just learned to make a prediction so it also takes those inputs the long term memory, and the new information joins them and decides what to output. 
The output becomes both the prediction and the new short term memory.  

We have long term memory and short term memory coming in which we call LTM and STM. And then an event and an output are coming in and out of LSTM. And then this passes to the next node, and so on and so forth. 

## Architecture of LSTM

### Architecture of RNN
Basically what we do is we take our event E_t and our memory M_t-1 coming from the previous point in time, and we apply a simple tanh or sigmoid activation function to obtain the output and then your memory M_t. So to be more specific, we join these two vectors and multiply them by a matrix W and add a bias b, and then squish this with the tanh function, and that gives us the output M_t. This output is a prediction and also the memory that we carry to the next node. 

### Architecture of LSTM 
The LSTM architecture is very similar, __except with a lot more nodes inside with two inputs and output since it keeps track of the long- and short-term memories.__ Short term memory is the output or prediction. 



## The Learn Gate
What the learn gate does is the following :

It takes a short term memory and the event and it joins it. Actually, it takes the short term memory and the event and it combines them and then it ignores a bit of it keeping the important part of it. 

How does this work mathmetically?

STM_t-1 and E_t and it combines them by putting them through a linear function which consists of joining the vectors multiplying by a matrix adding a bias and finally squishing the result with a tanh activation function. Then the new information N_t has this form over here. 
 
How we ignore part of it? by multiplying by an ignore factor i_t
The ignore factor, i_t, is actually a vector but it multiplies element wise.
How we calculate i_t? use our previous information of the short term memory and the event. we create a small neural network whose inputs are the short term memory and the event. We'll pass them through a small linear function with a new matrix and  a new bias and squish them with the sigmoid fuction to keep it between zero and one. 


## The Forget Gate

The forget gate works as follows :

It takes a long term memory and it decides what parts to keep and to forget.  

How does the forget gate work mathmetically?

LTM_t-1 comes in, and it gets multiplied by a __Forget Factor f_t__

How does the forget factort f_t get calculate? 

Use STM and the event information to calculate f_t. So, just as before, run a small one layer neural network with a linear function combined with sigmoid fucion to calculate this Forget Factor and that's how the Forget Gate works.



## The Remember Gate

It takes LTM coming out of the Forget Gate and STM coming out of the Learn Gate simply combines them together. Take the outputs coming from the Forget Gate and from the Learn Gate and we just __add them.__ 




## The Use gate 

This is the one that uses the long term memory that just cames out of the forget gate and the short term memory that just came out of learnd gate, to come up with a new short term memory and an output. 

Take what's useful from the LTM  and what's useful from the STM, and that's going to be our new short term memory. 

Mathmetically what this does is the following : 

it applies a small neural network on the output of the forget gate using the tanh actication function, and it applies to another small neural network on the short term memory and the events using the sigmoid activation function. And the finally steps, it multiplies these two in order to get the new output. The output also worth of the new short term memory.





## Other architectures

#### GRU
* [Michael Guerzhoy's post](http://www.cs.toronto.edu/~guerzhoy/321/lec/W09/rnn_gated.pdf)




















