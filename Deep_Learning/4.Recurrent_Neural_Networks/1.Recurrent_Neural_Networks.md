# Recurrent Neural Networks


## RNN Example


### Recurrent Neural Networks
So far, we've been looking at convolutional neural networks and models that allows us to analyze the spatial information in a given input image. CNN's excel in tasks that rely on finding spatial and visible patterns in training data.

In this and the next couple lessons, we'll be reviewing RNN's or recurrent neural networks. These networks give us a way to incorporate __memory__ into our neural networks, and will be critical in analyzing sequential data. RNN's are most often associated with text processing and text generation because of the way sentences are structured as a sequence of words!

### RNN's and Text Generation
At the end of this lesson, you will be tasked with creating a TV script generator; a model that takes in a series of words as input and outputs a likely next word, forming a text, one word at a time.

Similarly, RNN's can be used to generate text given any other data. For example, you can give an RNN a feature vector from an image, and use it to [generate a descriptive caption](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning). Image captions are used to create accessible content and in a number of cases where one may want to read about the contents of an image.

Sketch RNN
One of my favorite use cases for RNN's is in generating drawings. [Sketch RNN (demo here)](https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html) is a program that learns to complete a drawing, once you give it something (a line or circle, etc.) to start!


Sketch RNN example output. Left, Mona Lisa. Right, pineapple.

It's interesting to think of drawing as a sequential act, but it is! This network takes a starting line or squiggle and then, having trained on a number of types of sketches, does its best to complete the drawing based on your input squiggle.

Next, you'll learn all about how RNN's are structured and how they can be trained! This section is taught by Ortal, who has a PhD in Computer Engineering and has been a professor and researcher in the fields of applied cryptography and embedded systems.


## RNN Introduction


### RNN Introduction
Hi! I am Ortal, your instructor for this lesson!

In this lesson we will learn about Recurrent Neural Networks (RNNs).

The neural network architectures you've seen so far were trained using the current inputs only. We did not consider previous inputs when generating the current output. In other words, our systems did not have any memory elements. RNNs address this very basic and important issue by using memory (i.e. past inputs to the network) when producing the current output.


RNNs are artificial neural networks, that can capture temporal dependencies, which are dependencies over time. 

Recurrent : Occurring often or repeatedly

RNN : We perform the same task for each element in input sequence

RNNs are based on very similar ideas to those behind feedforward networks.

Feedforwad networks :

* Nonlinear Function Approximation
* Training
  - Backpropagation
  - Stochastic Gradient Descent
* Evaluation

RNNs :
* Applications
* Simple RNN
  -ELMAN network
* Training RNNs
* LSTM - Long-Short-Term-Memory



## RNN History

After the first wave of artificial neural networks in the mid 80s, it became clear that feedforward networks are limited since they are unable to capture temporarl dependencies, depencencies that change over time. 

Modeling temporal data is critical in most real-world applications, since natural signals like speech and video have time varying properties and are characterized by having dependencies across time. 

Time Delay Neural Network (TDNN 1989)
Simple RNN/ ELMAN Network (1990)
Long Short Term Memory (LSTM mid 1990's)

As mentioned in this video, RNNs have a key flaw, as capturing relationships that span more than 8 or 10 steps back is practically impossible. This flaw stems from the __"vanishing gradient"__ problem in which the contribution of information decays geometrically over time.

What does this mean?

As you may recall, while training our network we use __backpropagation__. In the backpropagation process we adjust our weight matrices with the use of a gradient. In the process, gradients are calculated by continuous multiplications of derivatives. The value of these derivatives may be so small, that these continuous multiplications may cause the gradient to practically "vanish".

__LSTM__ is one option to overcome the Vanishing Gradient problem in RNNs.

Please use these resources if you would like to read more about the [Vanishing Gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) or understand further the concept of a [Geometric Series](https://socratic.org/algebra/exponents-and-exponential-functions/geometric-sequences-and-exponential-functions) and how its values may exponentially decrease.

If you are still curious, for more information on the important milestones mentioned here, please take a peek at the following links:

* [TDNN](https://en.wikipedia.org/wiki/Time_delay_neural_network)
* Here is the original [Elman Network](http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/abstract) publication from 1990. This link is provided here as it's a significant milestone in the world on RNNs. To simplify things a bit, you can take a look at the following [additional info](https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks).
* In this [LSTM link](http://www.bioinf.jku.at/publications/older/2604.pdf) you will find the original paper written by [Sepp Hochreiter](https://en.wikipedia.org/wiki/Sepp_Hochreiter) and [JÃ¼rgen Schmidhuber](http://people.idsia.ch/~juergen/). Don't get into all the details just yet. We will cover all of this later!

As mentioned in the video, Long Short-Term Memory Cells (LSTMs) and Gated Recurrent Units (GRUs) give a solution to the vanishing gradient problem, by helping us apply networks that have temporal dependencies. In this lesson we will focus on RNNs and continue with LSTMs. We will not be focusing on GRUs. More information about GRUs can be found in the following [blog](https://skymind.ai/wiki/lstm). Focus on the overview titled: GRUs.



## RNN Applications


### Applications

The world's leading tech companies are all using RNNs, particularly LSTMs, in their applications. Let's take a look at a few.



There are so many interesting applications, let's look at a few more!

* Are you into gaming and bots? Check out the [DotA 2 bot by Open AI](https://openai.com/blog/dota-2/)
* How about [automatically adding sounds to silent movies](https://www.youtube.com/watch?time_continue=1&v=0FW99AQmMc8)?
* Here is a cool tool for [automatic handwriting generation](http://www.cs.toronto.edu/~graves/handwriting.cgi?text=My+name+is+Luka&style=&bias=0.15&samples=3)
* Amazon's voice to text using high quality speech recognition, [Amazon Lex](https://aws.amazon.com/ko/lex/faqs/).
* Facebook uses RNN and LSTM technologies for [building language models](https://code.facebook.com/posts/1827693967466780/building-an-efficient-neural-language-model-over-a-billion-words/)
* Netflix also uses RNN models - here is an [interesting read](https://arxiv.org/pdf/1511.06939.pdf)



## Feedforward Neural Network-Reminder


### Feedforward Neural Network - A Reminder

The mathematical calculations needed for training RNN systems are fascinating. To deeply understand the process, we first need to feel confident with the vanilla FFNN system. We need to thoroughly understand the feedforward process, as well as the backpropagation process used in the training phases of such systems. The next few videos will cover these topics, which you are already familiar with. We will address the feedforward process as well as backpropagation, using specific examples. These examples will serve as extra content to help further understand RNNs later in this lesson.

The following couple of videos will give you a brief overview of the __Feedforward Neural Network (FFNN)__.

After we have a clear understanding of the mathetical background (Partial derivatives, Linear algebra, Stochastic gradient descent, Backpropagation, Feeoforward neural network) as well, we will take another step and open the door to RNNs.

#### Combining CNNs with RNNs

one can use CNNs in the first few layers for the purpose of feature extraction, and then use RNNs in the final layer where memory needs to be considered. 

A popular application for this is in __gesture recognition.__

#### Feedforward Neural Network

Simulate an artificial neural network by using a nonlinear function approximation. That fuction will act as a system that has n number of inputs, weights, and k number of outputs. 

x : input vector

y : output vector

When neural network essentially works as a nonlinear function approximator, what we do is we try to fit a smooth function between given points.

#### Two main type of Applications

* Classification
Identify which of a set of categories a new input belongs to .
  * Image classification
 
* Regression 
Approximate a function, so the network produces continuous values following a supervised training process.
  * Time Series Forecasting
  
  
#### The Task in Neural Networks

Our task in neural networks is to find the best set of weights that yielda good output where x represents the inputs , W represent the wieghts. We start with random weights. 

In feedforward network, we have static mapping from the input to the outputs. We use the word static as we have no memory and the output depends only on the inputs and the weights. In other words, for the same input and the same weights, we always receive the same output. 

#### Two Primary Phases
* Training
 We tate the data set called the training set which includes many pairs of inputs and their corresponding targets or outputs. And the goal is to find a set of weights that would best map the inputs to the desired outputs. In other words, __the goal ot the training phase is to yield a network that generalizes beyond the train set.__
 
* Evaluation
 We use the network that was created in the training phase, apply our new inputs, and expect to obtain tha desired outputs. 

During the __training__ phase, we take the data set (also called the training set), which includes many pairs of inputs and their corresponding targets (outputs). Our goal is to find a set of weights that would best map the inputs to the desired outputs. In the __evaluation__ phase, we use the network that was created in the training phase, apply our new inputs and expect to obtain the desired outputs.

 The training phase will include two steps:

__Feedforward__

and

__Backpropagation

We will repeat these steps as many times as we need until we decide that our system has reached the best set of weights, giving us the best possible outputs.

The next two videos will focus on the feedforward process.

You will notice that in these videos I use subscripts as well as superscript as a numeric notation for the weight matrix.

For example:

* W_k is weight matrix _k
* W_{ij}^k is the ij element of weight matrix _k



## The Feedforward Process


### Feedforward
In this section we will look closely at the math behind the feedforward process. With the use of basic Linear Algebra tools, these calculations are pretty simple!

If you are not feeling confident with linear combinations and matrix multiplications, you can use the following links as a refresher:

* [Linear Combination](http://linear.ups.edu/html/section-LC.html)
* [Matrix Multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication)

Assuming that we have a single hidden layer, we will need two steps in our calculations. The first will be calculating the value of the hidden states and the latter will be calculating the value of the outputs.

Notice that both the hidden layer and the output layer are displayed as vectors, as they are both represented by more than a single neuron.

The two error functions that are most commonly used are the [Mean Squared Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) (usually used in regression problems) and the [cross entropy](https://www.ics.uci.edu/~pjsadows/notes.pdf) (usually used in classification problems).





## Feedforward Quiz


The following picture is of a feedforward network with

* A single input x
* Two hidden layers
* A singe output

The first hidden layer has M neurons.

The second hidden layer has N neurons.


### Quiz : What is the total number of multiplication operations needed for a single feedforward pass?


### Solution

To calculate the number of multiplications needed for a single feedforward pass, we can break down the network to three steps:

* Step 1: From the single input to the first hidden layer
* Step 2: From the first hidden layer to the second hidden layer
* Step 2: From the second hidden layer to the single output

__Step 1

The single input is multiplied by a vector with M values. Each value in the vector will represent a weight connecting the input to the first hidden layer. Therefore, we will have M multiplication operations.

__Step 2

Each value in the first hidden layer (M in total) will be multiplied by a vector with N values. Each value in the vector will represent a weight connecting the neurons in the first hidden layer to the neurons in the second hidden layer. Therefore, we will have here M times N calculations, or simply MN multiplication operations.

__Step 3

Each value in the second hidden layer (N in total) will be multiplied once, by the weight element connecting it to the single output. Therefore, we will have N multiplication operations.

In total, we will add the number of operations we calculated in each step: __M+MN+N .



## Backpropagation Theory

### Backpropagation Theory
Since partial derivatives are the key mathematical concept used in backpropagation, it's important that you feel confident in your ability to calculate them. Once you know how to calculate basic derivatives, calculating partial derivatives is easy to understand.
For more information on partial derivatives use the following [link](http://www.columbia.edu/itc/sipa/math/calc_rules_multivar.html)

For calculation purposes in future quizzes of the lesson, you can use the following link as a reference for [common derivatives.](http://tutorial.math.lamar.edu/pdf/Common_Derivatives_Integrals.pdf)

In the backpropagation process we minimize the network error slightly with each iteration, by adjusting the weights. The following video will help you understand the mathematical process we use for computing these adjustments.


Here you can find other good resources for understanding and tuning the Learning Rate:

* [resource 1](http://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/)
* [resource 2](http://cs231n.github.io/neural-networks-3/#loss)

The following video is given as a refresher to overfitting . You have already seen this concept in the Training Neural Networks lesson. Feel free to skip it and jump right into the next video.


#### Two solutions for the overfitting Problem

* Stop training process early

* Regularization
  * Dropout




The error is their squared difference, E=(d-y)^2, and is also called the network's Loss Function. We are dividing the error term by 2 to simplify notation, as will become clear soon.

The aim of the backpropagation process is to minimize the error, which in our case is the Loss Function. To do that we need to calculate its partial derivative with respect to all of the weights.






## RNN 

We are finally ready to talk about Recurrent Neural Networks (or RNNs), where we will be opening the doors to new content!

__Recurrent__ : occurring often or repeatedly

With RNN's , we perform the same task for each element in the input sequence. RNN's also attempt to address the need for capturing information and previous inputs by maintaining internal memory elements, also known as States.  

Many applications have temporal dependencies. Meaning that the current output depends not only on the current input, but also on a memory element which takes into account past inputs. 

* Sentiment Analysis
* Speech Recognition
* Time Series Prediction
* Natural Language Processing
* Gesture Recognition






RNNs are based on the same principles as those behind FFNNs, which is why we spent so much time reminding ourselves of the feedforward and backpropagation steps that are used in the training phase.

There are two main differences between FFNNs and RNNs. The Recurrent Neural Network uses:

* sequences as inputs in the training phase, and
* memory elements

Memory is defined as the output of hidden layer neurons, which will serve as additional input to the network during next training step.

The basic three layer neural network with feedback that serve as memory inputs is called the __Elman Network__

As mentioned in the History concept, here is the original [Elman Network](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1) publication from 1990. This link is provided here as it's a significant milestone in the world on RNNs. To simplify things a bit, you can take a look at the following [additional info](https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks).

Let's continue now to the next video with more information about RNNs.




## RNN - Unfolded Model 


















