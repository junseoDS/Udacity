# Convolutional Neural Networks


## Introducing Alexis

[@alexis_b_cook](https://twitter.com/alexis_b_cook)


## Applications of CNNs

" CNNs achieve state of the art results in a variety of proble areas including Voice User Interfaces, Natural Language Process, and Computer vision."

### Optional Resources
* Read about [the WaveNet model](https://deepmind.com/blog/wavenet-generative-model-raw-audio/).

  * Why train an A.I. to talk, when you can train it to sing ;)? In April 2017, researchers used a variant of the WaveNet model to generate songs. The original paper and demo can be found here.


* Learn about CNNs for [text classification](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/).

  * You might like to sign up for the author's [Deep Learning Newsletter](https://www.getrevue.co/profile/wildml)!


* Read about Facebook's [novel CNN approach](https://code.fb.com/ml-applications/a-novel-approach-to-neural-machine-translation/) for language translation that achieves state-of-the-art accuracy at nine times the speed of RNN models.


* Play [Atari games](https://deepmind.com/research/dqn/) with a CNN and reinforcement learning. You can [download](https://sites.google.com/a/deepmind.com/dqn/) the code that comes with this paper.

  * If you would like to play around with some beginner code (for deep reinforcement learning), you're encouraged to check out Andrej Karpathy's [post](http://karpathy.github.io/2016/05/31/rl/).


* Play [pictionary](https://quickdraw.withgoogle.com/#) with a CNN!

  * Also check out all of the other cool implementations on the [A.I. Experiments website](https://experiments.withgoogle.com/collection/ai). Be sure not to miss [AutoDraw](https://www.autodraw.com/)!


* Read more about [AlphaGo](https://deepmind.com/research/alphago/).

  * Check out [this article](https://www.technologyreview.com/s/604273/finding-solace-in-defeat-by-artificial-intelligence/?set=604287), which asks the question: If mastering Go “requires human intuition,” what is it like to have a piece of one’s humanity challenged?

* Check out these really cool videos with drones that are powered by CNNs.

  * Here's an interview with a startup - [Intelligent Flying Machines (IFM)](https://www.youtube.com/watch?v=AMDiR61f86Y).
  * Outdoor autonomous navigation is typically accomplished through the use of the [global positioning system (GPS)](https://www.droneomega.com/gps-drone-navigation-works/), but here's a demo with a CNN-powered [autonomous drone](https://www.youtube.com/watch?v=wSFYOw4VIYY).


* If you're excited about using CNNs in self-driving cars, you're encouraged to check out:

  * our [Self-Driving Car Engineer Nanodegree](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013), where we classify signs in the [German Traffic Sign dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset) in this [project](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project).
  * our [Machine Learning Engineer Nanodegree](https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009t), where we classify house numbers from the [Street View House Numbers](http://ufldl.stanford.edu/housenumbers/) dataset in this [project](https://github.com/udacity/machine-learning/tree/master/projects/digit_recognition).
  * this [series of blog posts](https://pythonprogramming.net/game-frames-open-cv-python-plays-gta-v/) that details how to train a CNN in Python to produce a self-driving A.I. to play Grand Theft Auto V.

* Check out some additional applications not mentioned in the video.

  * Some of the world's most famous paintings have been [turned into 3D](https://www.businessinsider.com/3d-printed-works-of-art-for-the-blind-2016-1) for the visually impaired. Although the article does not mention how this was done, we note that it is possible to use a CNN to [predict depth](https://cs.nyu.edu/~deigen/depth/) from a single image.
  * Check out [this research](https://ai.googleblog.com/2017/03/assisting-pathologists-in-detecting.html) that uses CNNs to localize breast cancer.
  * CNNs are used to [save endangered species](https://blogs.nvidia.com/blog/2016/11/04/saving-endangered-species/?adbsc=social_20170303_70517416)!
  * An app called [FaceApp](https://www.digitaltrends.com/photography/faceapp-neural-net-image-editing/) uses a CNN to make you smile in a picture or change genders.
  
  
  
## Lesson Outline

[Cezanne Camacho](https://twitter.com/cezannecam)


### What is a feature?
I’ve found that a helpful way to think about what a feature is, is to think about what we are visually drawn to when we first see an object and when we identify different objects. For example, what do we look at to distinguish a cat and a dog? The shape of the eyes, the size, and how they move are just a couple of examples of visual features.

As another example, say we see a person walking toward us and we want to see if it’s someone we know; we may look at their face, and even further their general shape, eyes (and even color of their eyes). The distinct shape of a person and their eye color a great examples of distinguishing features!

Next, we’ll see that features like these can be measured, and represented as numerical data, by a machine.


## MNIST Dataset

### MNIST Dataset

* 70,000 images of hand-written digits
* Famous database in machine learning

The MNIST database is arguably the most famous database in the field of deep learning! Check out this [figure](https://www.kaggle.com/benhamner/popular-datasets-over-time) that shows datasets referenced over time in [NIPS](https://nips.cc/) papers.


    # Am I missing a dataset? Fork this and add it below!
    # Also, identifying whether a dataset is referenced by a paper could be improved dramatically

    library(dplyr)
    library(ggplot2)
    library(ggrepel)
    library(readr)
    library(stringr)
    library(tidyr)

    papers <- read_csv("../input/papers.csv")

    papers$paper_text_no_refs <- str_c(str_sub(papers$paper_text, 1, floor(str_length(papers$paper_text)/2)), 
                                       sub("references.*", "references", 
                                           str_sub(papers$paper_text, floor(str_length(papers$paper_text)/2)+1,-1),
                                           ignore.case=TRUE))

    dataset_names <- c("MNIST", "CIFAR", "SVHN", "PASCAL", "KITTI", "TFD", "SensIT", "Connect4",
                     "adult", "credit", "kr-vs-kp", "promoters", "votes", "UCI", "digg", "HepTH",
                     "citeseer", "MovieLens", "RocketFuel", "tweet", "twitter", "bAbI", "TreeBank",
                     "SARCOS", "NORB", "TIMIT", "ImageNet", "Street View", "VGG", "Caltech-101",
                     "FM-IQA", "AP News", "newsgroups", "diabetes", "HES", "prostate", "MS COCO",
                     "Toronto Face", "glaucoma", "Alzheimer’s", "news20", "scleroderma",
                     "puzzle", "MADELON", "ENRON", "WIPO", "reuters", "CelebA", "Text8",
                     "Protein", "STL10")

    data_references <- papers[,"year"]
    for (dataset_name in dataset_names) {
      data_references[dataset_name] <- 0
      matches <- grep(str_c(" ", dataset_name, "[^a-z]"), papers$paper_text_no_refs, ignore.case = TRUE)
      data_references[matches, dataset_name] <- 1
    }

    data_references_by_year <- data_references %>%
      gather("dataset", "isreferenced", 2:ncol(data_references)) %>%
      group_by(year, dataset) %>%
      summarise(references=sum(isreferenced))

    popular_datasets <- unique(data_references_by_year[data_references_by_year$references>=10,]$dataset)
    popular_data_references_by_year <- data_references_by_year[data_references_by_year$dataset %in% popular_datasets,]

    p <- ggplot(popular_data_references_by_year,
           aes(x=year, y=references, group=dataset, color=dataset)) +
      geom_line() +
      geom_text_repel(data=popular_data_references_by_year[popular_data_references_by_year$year==2017,],
                     aes(x=year, y=references, label=dataset)) +
      ggtitle("Popular Dataset References Over Time") +
      xlab("Year") +
      ylab("Number of Papers Referencing the Dataset") +
      theme_light(base_size=14) +
      theme(legend.title=element_blank())
    ggsave("popular_data_references_by_year.png", p, height=6, width=10, units="in")

    write_csv(popular_data_references_by_year, "popular_data_references_by_year.csv")






## How Computers Interpret Images

White pixel : 255 -> 1.0

Black pixel : 0 -> 0

__Flattening__


### Normalizing image inputs
Data normalization is an important pre-processing step. It ensures that each input (each pixel value, in this case) comes from a standard distribution. That is, the range of pixel values in one input image are the same as the range in another image. This standardization makes our model train and reach a minimum error, faster!

Data normalization is typically done by subtracting the mean (the average of all pixel values) from each pixel, and then dividing the result by the standard deviation of all the pixel values. Sometimes you'll see an approximation here, where we use a mean and standard deviation of 0.5 to center the pixel values. [Read more about the Normalize transformation in PyTorch](https://pytorch.org/docs/stable/torchvision/transforms.html#transforms-on-torch-tensor).

The distribution of such data should resemble a [Gaussian function](http://mathworld.wolfram.com/GaussianFunction.html) centered at zero. For image inputs we need the pixel numbers to be positive, so we often choose to scale the data in a normalized range [0,1].




## MLP Structures & Class Scores

Create a neural network for discovering the patterns in our data.

### Class score 
Indicate how sure the network is that a given input is of a specific class


## Do your Research

Search mlp for mnist

Keep looking and see if you can find another structure that appeals to you

When you do find a model or two that look interesting, try them out in code and see how well they perform.



## Loss & Optimization

#### Learn from Mistakes

1. Loss : Measure any mistakes between a predicted and true class

2. Backpropagation : Quantify how bad a particular weight is in making a mistake

3. Optimization : Gives us a way to calculate a better weight value. 



## Defining a Network in PyTorch

### ReLU Activation Function

The purpose of an activation function is to scale the outputs of a layer so that they are a consistent, small value. Much like normalizing input values, this step ensures that our model trains efficiently!

A ReLU activation function stands for "Rectified Linear Unit" and is one of the most commonly used activation functions for hidden layers. It is an activation function, simply defined as the positive part of the input, x. So, for an input image with any negative pixel values, this would turn all those values to 0, black. You may hear this referred to as "clipping" the values to zero; meaning that is the lower bound.


## Training the Network


### Cross-Entropy Loss

In the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#crossentropyloss), you can see that the cross entropy loss function actually involves two steps:

* It first applies a softmax function to any output is sees
* Then applies [NLLLoss](https://pytorch.org/docs/stable/nn.html#nllloss); negative log likelihood loss


Then it returns the average loss over a batch of data. Since it applies a softmax function, we do not have to specify that in the forward function of our model definition, but we could do this another way.

#### Another approach

We could separate the softmax and NLLLoss steps.

* In the forward function of our model, we would explicitly apply a softmax activation function to the output, x.

       ...
       ...
      # a softmax layer to convert 10 outputs into a distribution of class probabilities
      x = F.log_softmax(x, dim=1)

      return x

Then, when defining our loss criterion, we would apply NLLLoss

    # cross entropy loss combines softmax and nn.NLLLoss() in one single class
    # here, we've separated them
    criterion = nn.NLLLoss()

This separates the usual criterion = nn.CrossEntropy() into two steps: softmax and NLLLoss, and is a useful approach should you want the output of a model to be class probabilities rather than class scores.



## Pre-Notebook : MLP Classification, Exercise


### Notebook: MLP Classification

Now, you're ready to define and train an MLP in PyTorch. As you follow along this lesson, you are encouraged to open the referenced Jupyter notebooks. We will present a solution to you, but please try creating your own deep learning models! Much of the value in this experience will come from experimenting with the code, in your own way.

To open this notebook, you have two options:

> * Go to the next page in the classroom (recommended).
> * Clone the repo from Github and open the notebook mnist_mlp_exercise.ipynb in the convolutional-neural-networks > mnist-mlp folder. You can either download the repository with git clone https://github.com/udacity/deep-learning-v2-pytorch.git, or download it as an archive file from this link.


### Instructions

* Define an MLP model for classifying MNIST images
* Train it for some number of epochs and test your model to see how well it generalizes and measure its accuracy.



## One Solution

model.eval()

There is an omission in the above code: including model.eval() !

model.eval() will set all the layers in your model to evaluation mode. This affects layers like dropout layers that turn "off" nodes during training with some probability, but should allow every node to be "on" for evaluation. So, you should set your model to evaluation mode before testing or validating your model and set it to model.train() (training mode) only during the training loop.

This is reflected in the previous notebook code and in our Github repository.

### Optional Resources
Check out the [first research paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) to propose dropout as a technique for overfitting.
If you'd like more information on activation functions, check out [this website](http://cs231n.github.io/neural-networks-1/#actfun).


## Model Validation

Traing Set : Update the model weights

Validation Set : check how well the model generalizes

Test Set : check accuracy of the trained model


## Validation Loss

### Validation Set: Takeaways

We create a validation set to

1. Measure how well a model generalizes, during training
2. Tell us when to stop training a model; when the validation loss stops decreasing (and especially when the validation loss starts increasing and the training loss is still decreasing)


## Image Classification Steps

1. Visualize Data

2. Pre-Process : Normalize Transform

3. Define a Model : Do research

4. Train your Model : Define loss & optimization functions

5. Save the Best Model : Consider using a validation dataset

6. Test Your Model





## MLP vs CNN


### Classifier Performance

* Check out the performance of [other classifiers](http://yann.lecun.com/exdb/mnist/).

## Loacal Connectivity

MLPs
* Only use fully connected layers
* Only accept vectors as input

CNNs
* Also use sparsely connected layers
* Also accept matrices as input



## Filters and the Convolutional Layer

A convolutional layer applies a series of different image filters also known as convolutional kernels to an input image. The resulting filtered images have different appearances.




## Filters & Edges


### Filters
To detect changes in intensity in an image, you’ll be using and creating specific image filters that look at groups of pixels and react to alternating patterns of dark/light pixels. These filters produce an output that shows edges of objects and differing textures.

So, let’s take a closer look at these filters and see when they’re useful in processing images and identifying traits of interest.



## Frequency in images

We have an intuition of what frequency means when it comes to sound. High-frequency is a high pitched noise, like a bird chirp or violin. And low frequency sounds are low pitch, like a deep voice or a bass drum. For sound, frequency actually refers to how fast a sound wave is oscillating; oscillations are usually measured in cycles/s (Hz), and high pitches and made by high-frequency waves. Examples of low and high-frequency sound waves are pictured below. On the y-axis is amplitude, which is a measure of sound pressure that corresponds to the perceived loudness of a sound, and on the x-axis is time.



#### High and low frequency

Similarly, frequency in images is a rate of change. But, what does it means for an image to change? Well, images change in space, and a high frequency image is one where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next. A low frequency image may be one that is relatively uniform in brightness or changes very slowly. This is easiest to see in an example.



Most images have both high-frequency and low-frequency components. In the image above, on the scarf and striped shirt, we have a high-frequency image pattern; this part changes very rapidly from one brightness to another. Higher up in this same image, we see parts of the sky and background that change very gradually, which is considered a smooth, low-frequency pattern.


High-frequency components also correspond to the edges of objects in images, which can help us classify those objects.


## High-pass Filters


### Edge Handling
Kernel convolution relies on centering a pixel and looking at it's surrounding neighbors. So, what do you do if there are no surrounding pixels like on an image corner or edge? Well, there are a number of ways to process the edges, which are listed below. It’s most common to use padding, cropping, or extension. In extension, the border pixels of an image are copied and extended far enough to result in a filtered image of the same size as the original image.

Extend The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90° wedges. Other edge pixels are extended in lines.

Padding The image is padded with a border of 0's, black pixels.

Crop Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being slightly smaller, with the edges having been cropped.




## Open CV & Creating Custom Filters

### OpenCV

Before we jump into coding our own convolutional kernels/filters, I'll introduce you to a new library that will be useful to use when dealing with computer vision tasks, such as image classification: OpenCV!


OpenCV is a computer vision and machine learning software library that includes many common image analysis algorithms that will help us build custom, intelligent computer vision applications. To start with, this includes tools that help us process images and select areas of interest! The library is widely used in academic and industrial applications; from [their site](https://opencv.org/about/), OpenCV includes an impressive list of users: “Along with well-established companies like Google, Yahoo, Microsoft, Intel, IBM, Sony, Honda, Toyota that employ the library, there are many startups such as Applied Minds, VideoSurf, and Zeitera, that make extensive use of OpenCV.”

So, note, how we import cv2 in the next notebook and use it to create and apply image filters!

### Notebook: Custom Filters

The next notebook is called custom_filters.ipynb.

To open the notebook, you have two options:

> 
> * Go to the next page in the classroom (recommended).
> * Clone the repo from Github and open the notebook custom_filters.ipynb in the convolutional-neural-networks > conv-visualization folder. You can either download the repository with git clone https://github.com/udacity/deep-learning-v2-pytorch.git, or download it as an archive file from this link.

### Instructions
* Define your own convolutional filters and apply them to an image of a road
* See if you can define filters that detect horizontal or vertical edges
This notebook is meant to be a playground where you can try out different filter sizes and weights and see the resulting, filtered output image!





## Convolutional Layer

### The Importance of Filters
What you've just learned about different types of filters will be really important as you progress through this course, especially when you get to Convolutional Neural Networks (CNNs). CNNs are a kind of deep learning model that can learn to do things like image classification and object recognition. They keep track of spatial information and learn to extract features like the edges of objects in something called a convolutional layer. Below you'll see an simple CNN structure, made of multiple layers, including this "convolutional layer".

### Convolutional Layer
The convolutional layer is produced by applying a series of many different image filters, also known as convolutional kernels, to an input image.

4 different filters produce 4 differently filtered output images. When we stack these images, we form a complete convolutional layer with a depth of 4!

### Learning
In the code you've been working with, you've been setting the values of filter weights explicitly, but neural networks will actually learn the best filter weights as they train on a set of image data. You'll learn all about this type of neural network later in this section, but know that high-pass and low-pass filters are what define the behavior of a network like this, and you know how to code those from scratch!

In practice, you'll also find that many neural networks learn to detect the edges of images because the edges of object contain valuable information about the shape of an object.


### Optional Resource
Check out [this website](http://setosa.io/ev/image-kernels/), which allows you to create your own filter. You can then use your webcam as input to a convolutional layer and visualize the corresponding activation map!



### Stride and Padding

Stride : the amount by which the filter slides over the image

A stride of one makes convolutional layer roughly the same width and height as the input image.

Instead, if we make the stride equal to two, the convolutional layer is about half the width and height of the image. 


The treatment of the edges 


## Pooling Layers
a method for reduciing dimensionality.
### Max Pooling
Max pooling layers will take a stack of feature maps as input.

### Other kinds of pooling
Alexis mentioned one other type of pooling, and it is worth noting that some architectures choose to use [average pooling](https://pytorch.org/docs/stable/nn.html#avgpool2d), which chooses to average pixel values in a given window size. So in a 2x2 window, this operation will see 4 pixel values, and return a single, average of those four values, as output!

This kind of pooling is typically not used for image classification problems because maxpooling is better at noticing the most important details about edges and other features in an image, but you may see this used in applications for which smoothing an image is preferable.



## Capsule Networks


### Alternatives to Pooling

It's important to note that pooling operations do throw away some image information. That is, they discard pixel information in order to get a smaller, feature-level representation of an image. This works quite well in tasks like image classification, but it can cause some issues.

Consider the case of facial recognition. When you think of how you identify a face, you might think about noticing features; two eyes, a nose, and a mouth, for example. And those pieces, together, form a complete face! A typical CNN that is trained to do facial recognition, should also learn to identify these features. Only, by distilling an image into a feature-level representation, you might get a weird result:

* Given an image of a face that has been photoshopped to include three eyes or a nose placed above the eyes, a feature-level representation will identify these features and still recognize a face! Even though that face is fake/contains too many features in an atypical orientation.

So, there has been research into classification methods that do not discard spatial information (as in the pooling layers), and instead learn to spatial relationships between parts (like between eyes, nose, and mouth).

> One such method, for learning spatial relationships between parts, is the capsule network.

### Capsule Networks

Capsule Networks provide a way to detect parts of objects in an image and represent spatial relationships between those parts. This means that capsule networks are able to recognize the same object, like a face, in a variety of different poses and with the typical number of features (eyes, nose , mouth) even if they have not seen that pose in training data.

Capsule networks are made of parent and child nodes that build up a complete picture of an object.


In the example above, you can see how the parts of a face (eyes, nose, mouth, etc.) might be recognized in leaf nodes and then combined to form a more complete face part in parent nodes.

### What are Capsules?

Capsules are essentially a collection of nodes, each of which contains information about a specific part; part properties like width, orientation, color, and so on. The important thing to note is that each capsule outputs a vector with some magnitude and orientation.

> * Magnitude (m) = the probability that a part exists; a value between 0 and 1.
> * Orientation (theta) = the state of the part properties.

These output vectors allow us to do some powerful routing math to build up a parse tree that recognizes whole objects as comprised of several, smaller parts!

The magnitude is a special part property that should stay very high even when an object is in a different orientation, as shown below.


### Resources
You can learn more about [capsules, in this blog post](https://cezannec.github.io/Capsule_Networks/).
And experiment with an implementation of a capsule network in PyTorch, [at this github repo](https://github.com/cezannec/capsule_net_pytorch).




## Increasing Depth

input image -> convolutional layer -> maxpooling layer -> fully connected layer -> predicted class


CNN will require a fixed size input. So pick an image size and resize all of data images to that size.
 




## CNNs for Image Classification


### Padding
Padding is just adding a border of pixels around an image. In PyTorch, you specify the size of this border.

Why do we need padding?

When we create a convolutional layer, we move a square filter around an image, using a center-pixel as an anchor. So, this kernel cannot perfectly overlay the edges/corners of images. The nice feature of padding is that it will allow us to control the spatial size of the output volumes (most commonly as we’ll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same).

The most common methods of padding are padding an image with all 0-pixels (zero padding) or padding them with the nearest pixel value. You can read more about calculating the amount of padding, given a kernel_size, [here](http://cs231n.github.io/convolutional-networks/#conv).


### PyTorch Layer Documentation

#### Convolutional Layers

We typically define a convolutional layer in PyTorch using nn.Conv2d, with the following parameters, specified:

    nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)

* in_channels refers to the depth of an input. For a grayscale image, this depth = 1
* out_channels refers to the desired depth of the output, or the number of filtered images you want to get as output
* kernel_size is the size of your convolutional kernel (most commonly 3 for a 3x3 kernel)
* stride and padding have default values, but should be set depending on how large you want your output to be in the spatial dimensions x, y
[Read more about Conv2d in the documentation.](https://pytorch.org/docs/stable/nn.html#conv2d)

#### Pooling Layers

Maxpooling layers commonly come after convolutional layers to shrink the x-y dimensions of an input, read more about pooling layers in PyTorch, [here](https://pytorch.org/docs/stable/nn.html#maxpool2d).



## Convolutional Layers in PyTorch

### Convolutional Layers in PyTorch

To create a convolutional layer in PyTorch, you must first import the necessary module:

    import torch.nn as nn

Then, there is a two part process to defining a convolutional layer and defining the feedforward behavior of a model (how an input moves through the layers of a network). First, you must define a Model class and fill in two functions.

#### init

You can define a convolutional layer in the __init__ function of by using the following format:

    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)

#### forward

Then, you refer to that layer in the forward function! Here, I am passing in an input image x and applying a ReLU function to the output of this layer.

    x = F.relu(self.conv1(x))

### Arguments

You must pass the following arguments:

* in_channels - The number of inputs (in depth), 3 for an RGB image, for example.
* out_channels - The number of output channels, i.e. the number of filtered "images" a convolutional layer is made of or the number of unique, convolutional kernels that will be applied to an input.
* kernel_size - Number specifying both the height and width of the (square) convolutional kernel.


There are some additional, optional arguments that you might like to tune:

* stride - The stride of the convolution. If you don't specify anything, stride is set to 1.
* padding - The border of 0's around an input array. If you don't specify anything, padding is set to 0.

NOTE: It is possible to represent both kernel_size and stride as either a number or a tuple.

There are many other tunable arguments that you can set to change the behavior of your convolutional layers. To read more about these, we recommend perusing the official [documentation](https://pytorch.org/docs/stable/nn.html#conv2d).


### Pooling Layers

Pooling layers take in a kernel_size and a stride. Typically the same value as is the down-sampling factor. For example, the following code will down-sample an input's x-y dimensions, by a factor of 2:

    self.pool = nn.MaxPool2d(2,2)

#### forward

Here, we see that poling layer being applied in the forward function.

x = F.relu(self.conv1(x))
x = self.pool(x)


#### Convolutional Example #1

Say I'm constructing a CNN, and my input layer accepts grayscale images that are 200 by 200 pixels (corresponding to a 3D array with height 200, width 200, and depth 1). Then, say I'd like the next layer to be a convolutional layer with 16 filters, each filter having a width and height of 2. When performing the convolution, I'd like the filter to jump two pixels at a time. I also don't want the filter to extend outside of the image boundaries; in other words, I don't want to pad the image with zeros. Then, to construct this convolutional layer, I would use the following line of code:

    self.conv1 = nn.Conv2d(1, 16, 2, stride=2)

#### Convolutional Example #2

Say I'd like the next layer in my CNN to be a convolutional layer that takes the layer constructed in Example 1 as input. Say I'd like my new layer to have 32 filters, each with a height and width of 3. When performing the convolution, I'd like the filter to jump 1 pixel at a time. I want this layer to have the same width and height as the input layer, and so I will pad accordingly. Then, to construct this convolutional layer, I would use the following line of code:

    self.conv2 = nn.Conv2d(16, 32, 3, padding=1)


### Sequential Models

We can also create a CNN in PyTorch by using a Sequential wrapper in the __init__ function. Sequential allows us to stack different types of layers, specifying activation functions in between!

    def __init__(self):
            super(ModelName, self).__init__()
            self.features = nn.Sequential(
                  nn.Conv2d(1, 16, 2, stride=2),
                  nn.MaxPool2d(2, 2),
                  nn.ReLU(True),

                  nn.Conv2d(16, 32, 3, padding=1),
                  nn.MaxPool2d(2, 2),
                  nn.ReLU(True) 
             )

#### Formula: Number of Parameters in a Convolutional Layer

The number of parameters in a convolutional layer depends on the supplied values of filters/out_channels, kernel_size, and input_shape. Let's define a few variables:

* K - the number of filters in the convolutional layer
* F - the height and width of the convolutional filters
* D_in - the depth of the previous layer

Notice that K = out_channels, and F = kernel_size. Likewise, D_in is the last value in the input_shape tuple, typically 1 or 3 (RGB and grayscale, respectively).

Since there are F*F*D_in weights per filter, and the convolutional layer is composed of K filters, the total number of weights in the convolutional layer is K*F*F*D_in. Since there is one bias term per filter, the convolutional layer has K biases. Thus, the number of parameters in the convolutional layer is given by K*F*F*D_in + K.

#### Formula: Shape of a Convolutional Layer

The shape of a convolutional layer depends on the supplied values of kernel_size, input_shape, padding, and stride. Let's define a few variables:

* K - the number of filters in the convolutional layer
* F - the height and width of the convolutional filters
* S - the stride of the convolution
* P - the padding
* W_in - the width/height (square) of the previous layer

Notice that K = out_channels, F = kernel_size, and S = stride. Likewise, W_in is the first and second value of the input_shape tuple.

The depth of the convolutional layer will always equal the number of filters K.

The spatial dimensions of a convolutional layer can be calculated as: (W_in−F+2P)/S+1


### Flattening

Part of completing a CNN architecture, is to flatten the eventual output of a series of convolutional and pooling layers, so that all parameters can be seen (as a vector) by a linear classification layer. At this step, it is imperative that you know exactly how many parameters are output by a layer.

For the following quiz questions, consider an input image that is 130x130 (x, y) and 3 in depth (RGB). Say, this image goes through the following layers in order:

    nn.Conv2d(3, 10, 3)
    nn.MaxPool2d(4, 4)
    nn.Conv2d(10, 20, 5, padding=2)
    nn.MaxPool2d(2, 2)



## Feature Vector

> The layers in a CNN convert an input image array into a representation that encodes only the content of the image
> This is often call a feature level representation of an image or feature vector.










