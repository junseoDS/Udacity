# Style Transfer


## Style Transfer

CNNs are some of the most powerful networks for image classification and analysis. 

CNN's process visual information in a feed forward manner, passing an input image through a collection of image filters which extract certain features from the input image.
It turns out that these feature level representaions not only useful for classification, but also for image construction as well. 
These representations are the basis for applications like Style Transfer and Deep Dream. Which compose images based on CNN layer activations and extracted features.

Style transfer allows you to apply the style of one image to another image of your choice. 



## Separating Style & Content

When a CNN is trained to classify images, its convolutional layers learn to extract more and more complex features from a given image. 
Intermittently, max pooling layers will discard detailed spartial information, information that's increasingly irrelevant to the task of classification. 
The input image is transformed into feature maps that increasingly care about the content of the image rather than any detail about the texture and color of pixels.
Later layers of network are even sometimes referred to as a content representation of an image.
In this way, a trained CNN has learned to represent the content of an image.

Style can be thought of as traits that might be found in the brush strokes of a painting, its texture, colors, curvature, and so on.
To perform Style transfer, we need to combine the content of one image with the stlye of another. 
To represent the style of an input image, a feature space designed to capture texture and color information is used.
This space enssentially looks at spatial correlations within a layer of a network. 

The similariyies and differences bewteen features in a layer should give us some information about the texture and color information found in an image.
But at the same time, it should leave out information about the actual arrangement and identity of differnet objects in that image.



## VGG19 & Content Loss


### Resources : 

* [Image Style Transfer Using Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)



## Gram Matrix

The style representation of an image relies on looking at correlations between the features in indiviual layers of the VGG19 network.
In other work looking at how similar the features in a single layer are.  
The correlations at each layers are given by a Gram matrix. 

* The first step in calculating the Gram matrix will be to vectorize the values in one piece of the feature map.
  By flattening the XY dimensions of the feature maps, we are converting a 3D convolutional layer into 2D matrix of values.
* The next step is to multiply this matrix by its transpose.
  Essentially, multiplying the features in each map to get the gram matrix. 
  This operation treats each value in the feature map as an individual sample, unrelated in space to other values. 
  So the resultant Gram matrix contains non-localizaed information about the layer.
  Non-localized information is information that would still be there even if an image was shuffled around in space 


## Style Loss


### Style loss

To calculate the style loss between a target and style image, we find the mean squared distance between the style and target image gram matrices, all five pairs that are computed at each layer in our predefined list, conv1_1 upto conv5_1.

Ss Ts

We'll multiply these five calculated distances by some style weights W that we specify, and then add them up. The style weights are values that will give more or less weight to the calculated style loss at each of the five layers, 
thereby changing how much effect each layer style representation will have on our final target image. 
Again we'll only be change the target image's style representations as we minimize this loss over some number of iterations. 

### Total Loss

* Content loss, which tells us how close the content of our target image is to that of our content image,

* Style loss, which tells us how close our targget is in style to our stlye image. 

Total loss = content loss + style loss

 Then use typical back propagation and optimization to reduce this loss by iteratively changing the target image to match our desired content and style.
 
 

## Loss Weights

We have values for the content and style loss, but because they're calculated differently, these values will be pretty different, and we want our target image to take both into account fairly equally.
So it's necessary to apply constan weights, alpha and beta, to the content and style losses, such that the total loss reflects an equal balance.

* alpha : Content weight
* beta : Style weight

In practice, this means multiplying the style loss by a much larger weight value than the content loss. 

Ratio of the content and style weights, alpha over beta ( a/b ) 

A smaller ratio corresponds to a larger value for beta, the style weight.






__Style_Transfer_Exercise.ipynb__










































