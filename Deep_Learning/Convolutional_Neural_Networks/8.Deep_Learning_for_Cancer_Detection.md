# Deep Learning For Cancer Detection

## Intro

### Sebastian Thrun 

The godfather of self-driving cars as Udacity co-founder.



## Skin Cancer

Skin cancer is one of the most common cancer in the world. It's not the most deadly but the most common.

In the US, 5.4 million new cases of skin cancer every year. 

There come in differnet types :

* Carcinomas
* Melanomas : the ones that typically kill peaple, it's called the black cancer.

20% of Americans will eventually get skin cancer, in most cases, benign cancer. 

Pre-cancer also called a Actinic Keratosis, affectes 58 million Americans and many more in the world. 




## Survival Probability of Skin Cancer

There are much difference between stage zero and stage 4.

Therefore, early detection is paramount to not dying from skin cancer. 



## Medical Classification

There's Charateristics that doctors use for finding melanomas :

* fuzziness of the boader,
* asymmetry
* coloration
* growth rate
 
 
 ## The data
 
 
 130,000 images of skin conditions from various data sources, including the web. and these images came with disease labels.
 All theses images were biopsied, so someone had acutally cut out the condition and done a a correct diagnostic. 
 It came with 2,000 different diseases, from inflammatory diseases, to rashes, to lesions, to all kinds of stuff
 
 
 
## Image Challenges 
 
### Challenge

Looking at the following images, could you tell the characteristics that determine if a lesion is benign (above) or malignant (below)?


## Quiz : Data Challenges

### Quiz : What challenges do you think we faced when cleaning up our data?

* Duplicates

* Variable resolution

* Variable lighting

* Big yellow markers

 
 Answer is all
 
 
 
 ## Training the Neural Network
 
 The architecture of the net was very standard. (Inception-v3)
 
 
 ## Quiz : Random vs Pre-initialized Weights
 
 ### Quiz : Do you think that pretraining the network with completely different images like cats, dogs, horses, and cars, made cancer classification better, the same, or worse?
 
 * Better
 * The same
 * Worse
 
 Answer : Better! 
 
 We had significantly better result within it was pretrain on completely different objects than if he trained enough from scratch.
 
Somehow, the features that develop inside his layers of neural network irrepective of what image that you are training on, have enough commonality that you get a better classifier with pretraining. 



## Validating the Training

To validate our results, we constructed an independent test set.

We got 72% accuracy in finding cancers, and Dermatolists got 66% accuracy.
 
 
 
 ## Sensitivity and Specificity
 
 you'll need to Google the definitions of sensitivity and specificity. If you'd like a refresher on precision and recall, here is [one resource](https://en.wikipedia.org/wiki/Precision_and_recall).
 
 
 __Sensitivity__ : Of all the sick people, how many did we diagnose as sick?
 
 __Specificity__ : Of all the health people, how may did we diagnose as health?
 


## More on Sensitivity and Specificity


### Sensitivity and Specificity
Although similar, sensitivity and specificity are not the same as precision and recall. Here are the definitions:

In the cancer example, sensitivity and specificity are the following:

* Sensitivity: Of all the people with cancer, how many were correctly diagnosed?
* Specificity: Of all the people without cancer, how many were correctly diagnosed?

And precision and recall are the following:

* Recall: Of all the people who have cancer, how many did we diagnose as having cancer?
* Precision: Of all the people we diagnosed with cancer, how many actually had cancer?

From here we can see that Sensitivity is Recall, and the other two are not the same thing.

Trust me, we also have a hard time remembering which one is which, so here's a little trick. If you remember from Luis's Evaluation Metrics section, here is the confusion matrix


Now, sensitivity and specificity are the rows of this matrix. More specifically, if we label

* TP: (True Positives) Sick people that we correctly diagnosed as sick.
* TN: (True Negatives) Healthy people that we correctly diagnosed as healthy.
* FP: (False Positives) Healthy people that we incorrectly diagnosed as sick.
* FN: (False Negatives) Sick people that we incorrectly diagnosed as healthy.

then:

Sensitivity = TP/(TP + FN) 

and

Specificity = TN/(TN + FP)

And precision and recall are the top row and the left column of the matrix:

Recall = TP/(TP + FN) 

and

Precision = TP/(TP + FP)



## ROC curves

### Area under a ROC curve

* Random model : 0.5

* Good model : 0.8

* Perfect model : 1 


Recall that the values in the horizontal axis are all the possible thresholds. For any threshold pp between 0 and 1, the verdict of the model will be the following: "Any lesion to the left of this threshold will be considered benign, and any lesion to the right of this threshold will be considered malignant, and sent for more tests."

Now, for this particular model, we calculate the sensitivity and specificity as follows:

Sensitivity: Out of all the malignant lesions, what percentage are to the right of the threshold (correctly classified)?
Specificity: Out of all the benign lesions, what percentage are to the left of the threshold (correctly classified)?
And we plot that point, where the coordinates are (Sensitivity, Specificity). If we plot all the points corresponding to each of the possible thresholds between 0% and 100%, we'll get the ROC curve that I drew above. Therefore, we can also refer to the ROC curve as the Sensitivity-Specificity Curve.

And finally, here's a little animation of the ROC curve getting drawn, as the threshold moves from 0 to 1.


## Comparing our Results with Doctors


## Visualization


## What is the network looking at?

What is network fixating on? What is actually look at? 

The fact that there are multiple black dots seems to have a big impact on the vote of the NN as shown by the corresponding dark areas in the right image. 



## Confusion Matrices

### Type 1 and Type 2 Errors

Sometimes in the literature, you'll see False Positives and True Negatives as Type 1 and Type 2 errors. Here is the correspondence:

* Type 1 Error (Error of the first kind, or False Positive): In the medical example, this is when we misdiagnose a healthy patient as sick.
* Type 2 Error (Error of the second kind, or False Negative): In the medical example, this is when we misdiagnose a sick patient as healthy.

But confusion matrices can be much larger than 2 \times 2 2×2. Here's an example of a larger one. Let's say we have three illnesses called A, B, C. And here is a confusion matrix



## Conclusion

We publish this result in the journal Nature.    

What is the future of medicine look like if a camera-based-system can outperfom the best human doctors?

Deep learning has the potential to massively change how the society works. 

Many of us, do the same repetitive task everyday , in and out. 

What AI can do with us, it can make us super human. 


## Useful Resources

Here's our publication in [Nature](https://www.nature.com/articles/nature21056.epdf?author_access_token=8oxIcYWf5UNrNpHsUHd2StRgN0jAjWel9jnR3ZoTv0NXpMHRAJy8Qn10ys2O4tuPakXos4UhQAFZ750CsBNMMsISFHIKinKDMKjShCpHIlYPYUHhNzkn6pSnOCt0Ftf6).

Other articles about our work:

* [Fortune Magazine](https://fortune.com/2017/01/26/stanford-ai-skin-cancer/)
* [Bloomberg](https://www.bloomberg.com/news/articles/2017-06-29/diagnosing-skin-cancer-with-google-images)
* [BBC](https://www.bbc.com/news/health-38717928)
* [Wall Street Journal](https://www.wsj.com/articles/computers-turn-medical-sleuths-and-identify-skin-cancer-1486740634?emailToken=JRrzcPt+aXiegNA9bcw301gwc7UFEfTMWk7NKjXPN0TNv3XR5Pmlyrgph8DyqGWjAEd26tYY7mAuACbSgWwvV8aXkLNl1A74KycC8smailE=)
* [Forbes](https://www.forbes.com/sites/forbestechcouncil/2017/09/27/what-can-computer-vision-do-in-the-palm-of-your-hand/#6722223247a7)
* [Scientific American](https://www.scientificamerican.com/article/deep-learning-networks-rival-human-vision1/)






## Mini Project : Dermatologist AI

### Introduction
In this mini project, you will design an algorithm that can visually diagnose [melanoma](https://www.skincancer.org/skin-cancer-information/melanoma), the deadliest form of skin cancer. In particular, your algorithm will distinguish this malignant skin tumor from two types of benign lesions ([nevi](http://missinglink.ucsf.edu/lm/dermatologyglossary/nevus.html) and [seborrheic keratoses](https://www.aad.org/public/diseases/bumps-and-growths/seborrheic-keratoses)).

The data and objective are pulled from the 2017 ISIC Challenge on Skin Lesion Analysis Towards Melanoma Detection. As part of the challenge, participants were tasked to design an algorithm to diagnose skin lesion images as one of three different skin diseases (melanoma, nevus, or seborrheic keratosis). In this project, you will create a model to generate your own predictions.

### Getting Started
1. Clone the [repository](https://github.com/udacity/dermatologist-ai) and create a data/ folder to hold the dataset of skin images.
    git clone https://github.com/udacity/dermatologist-ai.git
    mkdir data; cd data
2.Create folders to hold the training, validation, and test images.
    mkdir train; mkdir valid; mkdir test

3. Download and unzip the [training data](https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/train.zip) (5.3 GB).

4. Download and unzip the [validation data](https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/valid.zip) (824.5 MB).

5. Download and unzip the [test data](https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/test.zip) (5.1 GB).

6. Place the training, validation, and test images in the data/ folder, at data/train/, data/valid/, and data/test/, respectively. Each folder should contain three sub-folders (melanoma/, nevus/, seborrheic_keratosis/), each containing representative images from one of the three image classes.

You are free to use any coding environment of your choice to solve this mini project! In order to rank your results, you need only use a pipeline that culminates in a CSV file containing your test predictions.


### Create a Model

Use the training and validation data to train a model that can distinguish between the three different image classes. (After training, you will use the test images to gauge the performance of your model.)

If you would like to read more about some of the algorithms that were successful in this competition, please read [this article](https://arxiv.org/pdf/1710.05006.pdf) that discusses some of the best approaches. A few of the corresponding research papers appear below.

* Matsunaga K, Hamada A, Minagawa A, Koga H. [“Image Classification of Melanoma, Nevus and Seborrheic Keratosis by Deep Neural Network Ensemble”](https://arxiv.org/ftp/arxiv/papers/1703/1703.03108.pdf). International Skin Imaging Collaboration (ISIC) 2017 Challenge at the International Symposium on Biomedical Imaging (ISBI).
* Daz IG. [“Incorporating the Knowledge of Dermatologists to Convolutional Neural Networks for the Diagnosis of Skin Lesions”](https://arxiv.org/pdf/1703.01976.pdf). International Skin Imaging Collaboration (ISIC) 2017 Challenge at the International Symposium on Biomedical Imaging (ISBI). ([github](https://github.com/igondia/matconvnet-dermoscopy))
* Menegola A, Tavares J, Fornaciali M, Li LT, Avila S, Valle E. “[RECOD Titans at ISIC Challenge 2017”](https://arxiv.org/abs/1703.04819). International Skin Imaging Collaboration (ISIC) 2017 Challenge at the International Symposium on Biomedical Imaging (ISBI). ([github](https://github.com/learningtitans/isbi2017-part3))

While the original challenge provided additional data (such as the gender and age of the patients), we only provide the image data to you. If you would like to download this additional patient data, you may do so at the competition [website](https://challenge.kitware.com/#phase/5840f53ccad3a51cc66c8dab).

All three of the above teams increased the number of images in the training set with additional data sources. If you'd like to expand your training set, you are encouraged to begin with the [ISIC Archive](https://isic-archive.com/#images).

### Evaluation
Inspired by the ISIC challenge, your algorithm will be ranked according to three separate categories.

#### Category 1: ROC AUC for Melanoma Classification

In the first category, we will gauge the ability of your CNN to distinguish between malignant melanoma and the benign skin lesions (nevus, seborrheic keratosis) by calculating the area under the receiver operating characteristic curve ([ROC AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)) corresponding to this binary classification task.

If you are unfamiliar with ROC (Receiver Operating Characteristic) curves and would like to learn more, you can check out the documentation in [scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py) or read [this Wikipedia article](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).

The top scores (from the ISIC competition) in this category can be found in the image below.


#### Category 2: ROC AUC for Melanocytic Classification
All of the skin lesions that we will examine are caused by abnormal growth of either [melanocytes](https://en.wikipedia.org/wiki/Melanocyte) or [keratinocytes](https://en.wikipedia.org/wiki/Keratinocyte), which are two different types of epidermal skin cells. Melanomas and nevi are derived from melanocytes, whereas seborrheic keratoses are derived from keratinocytes.

In the second category, we will test the ability of your CNN to distinuish between melanocytic and keratinocytic skin lesions by calculating the area under the receiver operating characteristic curve ([ROC AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)) corresponding to this binary classification task.

The top scores in this category (from the ISIC competition) can be found in the image below.


#### Category 3: Mean ROC AUC
In the third category, we will take the average of the ROC AUC values from the first two categories.

The top scores in this category (from the ISIC competition) can be found in the image below.



### Getting your Results
Once you have trained your model, create a CSV file to store your test predictions. Your file should have exactly 600 rows, each corresponding to a different test image, plus a header row. You can find an example submission file (sample_submission.csv) in the repository.

Your file should have exactly 3 columns:

* Id - the file names of the test images (in the same order as the sample submission file)
* task_1 - the model's predicted probability that the image (at the path in Id) depicts melanoma
* task_2 - the model's predicted probability that the image (at the path in Id) depicts seborrheic keratosis

Once the CSV file is obtained, you will use the get_results.py file to score your submission. To set up the environment to run this file, you need to create (and activate) an environment with Python 3.5 and a few pip-installable packages:

    conda create --name derm-ai python=3.5
    source activate derm-ai
    pip install -r requirements.txt

Once you have set up the environment, run the following command to see how the sample submission performed:

    python get_results.py sample_predictions.csv

Check the terminal output for the scores obtained in the three categories:

    Category 1 Score: 0.526
    Category 2 Score: 0.606
    Category 3 Score: 0.566

The corresponding ROC curves appear in a pop-up window, along with the confusion matrix corresponding to melanoma classification.


As you can see from the confusion matrix, the sample submission currently predicts that most of the images in the test dataset correspond to benign lesions. Let's see if your model can improve these results, towards better detecting cancer!

The code for generating the confusion matrix assumes that the threshold for classifying melanoma is set to 0.5. To change this threshold, you need only supply an additional command-line argument when calling the get_results.py file. For instance, to set the threshold at 0.4, you need only run:

    python get_results.py sample_predictions.csv 0.4

To test your own submission, change the code to instead include the path to your CSV file.








