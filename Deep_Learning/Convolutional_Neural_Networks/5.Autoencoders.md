# Autoencoders


## Autoencoders

Along the way, the CNN is discarding Spatial information from the input image and isolating high level information about its content. 

Some of this structure can also be thought of as a kind of data compression; compressing from an image into something like a feature vector, which is basically a feature map produced after an input has gone through a series of layers squished into a vector shape.

This is part of what makes up something called an autoencoder.

An autoencoder has two main components :

* encoder : compresses some input data
* decoder : reconstructs data from the compressed representation.


Autoencoders are used in a traditional data compression sense, in that they can learn to reduce the dimensionality of any input. 
Then, anyonecan use a compressed representation to share it, or view it and so on, faster than they could with the original input.

Autoencoder learns efficient data compression and decompression functions instead of having them design, encoded by human. 

Autoencoder have shown the most promise in 

* image de-noising techniques
* filling in missing data 

Generally, the whole network is trained by minimizing the difference between the input and output. 



## A Linear Autoencoder

### A Simple Autoencoder

We'll start off by building a simple autoencoder to compress the MNIST dataset. With autoencoders, we pass input data through an encoder that makes a compressed representation of the input. Then, this representation is passed through a decoder to reconstruct the input data. Generally the encoder and decoder will be built with neural networks, then trained on example data.


### Compressed Representation

A compressed representation can be great for saving and sharing any kind of data in a way that is more efficient than storing raw data. In practice, the compressed representation often holds key information about an input image and we can use it for denoising images or oher kinds of reconstruction and transformation!



In this notebook, we'll be build a simple network architecture for the encoder and decoder. Let's get started by importing our libraries and getting the dataset.

### Linear Autoencoder

We'll train an autoencoder with these images by flattening them into 784 length vectors. The images from this dataset are already normalized such that the values are between 0 and 1. Let's start by building a simple autoencoder. The encoder and decoder should be made of **one linear layer**. The units that connect the encoder and decoder will be the _compressed representation_.

Since the images are normalized between 0 and 1, we need to use a **sigmoid activation on the output layer** to get values that match this input value range.



## Learnable Upsampling

Convolutional layers give us a way to preserve spatial information. So generally, they're part of a better and more elegant solution for encoding and decoding images. 

The encoder portion of an autoencoder is something that've you've seen before in our lesson on CNN's
It's made of a series of convolutional and maxpooling layers that downsample the spatial dimensions of an input image. 

In decoder, you want to resverse the downsampling process that happened in encoder. You want to increase the spatial dimensions of a compressed input to produce a reconstructed image that has the same shape as the origial input. Upsample an image by un-pooling the pixel values in an input. 

You can use an interpolation technique like

* Nearest Neighbors
Nearest neighbors expands a given area by copying over a single pixel value from an input image to , 2 by 2 in the larger output image. 

But a realistic large image is likely to have more variety in pixel values.


### Transpose Convolutional Layer

This layer does not use a predefined interpolation method, instead it has learnable parameters.
You'll sometimes hear these referre to as deconvolutional layers, but it's not strictly undoing a convolutional step.  




## Transpose Convolutions 












































































