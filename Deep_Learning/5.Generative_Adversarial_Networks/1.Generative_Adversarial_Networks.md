# Generative Adversarial Networks



## Introducing Ian Goodfellow

Ian is the inventor of Gerative Adversarial Networks, a major advancement in deep learning. 

* TA at Stanford
* Textbook co-authour
* Steering committee of Distill

### GANs

There are machine learning model that can imagine new things. 

If you give them a training set of somthing like images, they can make entirely new images that are realistic, even though they've never been seen before. 




## Applications of GANs

GANs are used for generating realistic data. Most of the application of GANs so far have been for images. 

For example, the __StackGAN model__ is really great at taking a textual description of a bird than generating a high resolution photo of a bird matching that description.  The GAN is drawing a sample from the probability distribution over all hyperthetical images matching that description, so you can keep running the GAN to get more images. 

A tool called __iGAN__ developed in collaboration between Berkeley and Adobe uses GANs to help artists. As the user draws very crude sketches using the mouse, iGAN searches for the nearest possible realistic image. 

GANs can also be used for image to image translation where images in one domain can be turned into images in another domain. Blueprints for building can be turned  into photos of finished buildings, or drawing of cats can be turned into realistic photos of cats. 

What's even more exciting is that image-to-image translation models can be trained in an unsupervised way. Facebook AI researchers showed how to train a model that can turn a photo of a face into a cartoon of a face. The model was trained on photos and on cartoons, but it did not need pairs of images showing which phto corresponds to which cartoon. 

Researchers from Nvidia showed how to use a similar GAN technique. For example, to turn photos of day scenes into photos of night scenes. At Berkeley, a model called CycleGAN is especially good at unsupervised image-to-image translation. 

GANs can also be used to create realistic simulated training sets or enviroments to train other machine learning models or reinforcement learning agents. 
Apple's first AI researcher paper was about using GANs to take 3D rendered images of eyes and change them to appear more realistic. 


GANs can also be used for imitation learning. We can think of the actions taken by a reinforcement learning agent as being a kind of data just as GANs can learn to imitate data like the images in training set of several photos, GANs can learn to imitate the action taken by a human expert. 


### Links to Resources

* [StackGAN](https://arxiv.org/abs/1612.03242) realistic image synthesis
* [iGAN](https://github.com/junyanz/iGAN) interactive image generation
* CartoonGAN, linked below

You'll learn much more about Pix2Pix and CycleGAN formulations, later in this program!





## How GANs work


Generative adversarial networks are a kind of generative model. 

Recurrent text models generate assignats one word ata time. It's also possible to make one word at a time, style model per images, where the model generates the image one pixel at a time. In general, this strategy is called, __'fully visible belief networks' going back to the '90s or auto regressive models__ as these models were renamed when they were rediscovered later. 

But, what if we would like to generate an entire output value such as entire image in one shot? GANs are kind of generative model that let's us generate a whole image in parallel. GANs use a differentiable function represented by a neural networks as a generator network. The generator network takes random noise as input, then runs that noise through a differentialbe function to transorm the noise and reshape it to have recognizable structure. The output of the generative network is a realistic image. The choice of the random input noise determines which image will come out of the generator network. Running the generator network with many diffrent input noise values produces many different realistic output images. The goal is for these images to be fair samples from the distribution over real data. Of course, the generator net doesn't start out producing realistic images. It has to be trained. The training process for a generative model is very different from the training process for a supervised learning model. __For a supervised learning model__, we show the model an image of a traffic light and we tell it, this is traffic light. __For generative model__ there's no output to associate with each image. We just show the model a lot of images and ask it to make more images that come from the same probability distribution.

But how do we actually get the model to do that? 
Most generative models are trained by adjusting the parameters to maximize the probability that the generator net will generate the training data set. 
Unfortunately for a lot of interesting models, it can be very difficult to compute this probability. Most generative models get around that with some kind of approximation. 

GANs use ab approximation where a second networks, called __the discriminator__, learns to guide the generator. The discriminator is just a regular neural net classifier, like you have all seen several times by now. During the training process, the discriminator shown real images from the training data half the time and fake images from the generator the other half of the time. The discriminator is trained to output the probability that the input is real. So it tries to assign a probability near 1 to real images, and a probability near zero to fake images. Meanwhile the generator tries to do the opposite. 
Over time, the generator is forced to produce more realistic output to fool the discriminator. 




You can find more information on the graph in the video in Figure 1 of https://arxiv.org/pdf/1406.2661.pdf.



## Games and Equilibria














