# Deep Convolutional GANs


## Deep Convolutional GANs

In this lesson we will be training to recognize numbers and photos of house addresses. 

### Street View House Number (SVHN) Dataset 
The numbers are of different colors and in differnet settings to MNIST

To get the best performance out of our model, we're going to use convolutional layers to make a deep convolutional GAN, usually called DCGAN. 
A DCGAN will still have a generator and discriminator networks, only this time these networks we made of convolutional layers that are designed to work with spatial data. The discriminator will be a kind of convolutional neural network that aims to classify any image it sees as either real training image or a fake generated image, and generator will be a transpose convolutional network that aims to upsample a laten vector z and generate images that can fool the discriminator. These networks have opposing goals and they're pulling the error rate of discriminator in different directions. 



 
 ## DCGAN, Discriminator
 
 A DCGAN is made of two networks, a generator and a discriminator.  
 
 ### Discriminator
 
 The discriminator is a convolutional neural network with one fully connected layer at the end. They should take in an image is input and output of value that indicates whether that image is real and from our training data or fake. There are __no maxpooling layers in this network__. Instead, the down sampling is done entirely with convolutional layers that have a stride equal to 2. 
 

 
 ### DCGAN Paper
It's always good to take a look at the original paper when you can. Many papers discuss both the theory and training details of deep learning networks, and you can read the DCGAN paper, [Unsupervised Representational Learning with Deep Convolutional Generative Adversarial Networks, at this link](https://arxiv.org/pdf/1511.06434.pdf).
 
 
 
 #### Architecture guidelines for stable Deep Convolutional GANs

 * Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions(generator)
 * Use batchnorm in both the generator and the discriminator.
 * Remove fully connected hidden layers for deeper architectures.
 * Use ReLU activation in generator for all layers except for the output, which uses Tanh.
 * Use LeakyReLu activation in the discriminator for all layers.
 
 
 
 
 
 ## DCGAN Generator
 
The generator is, I think the most interesting part of a GAN because its job is to really learn something about the underlying structure of training data. 
 
As before, the input to the generator is a random vector z, usually with somthing around a 100 values. The output of generator, it needs to be an image that can be sent to the discriminator. That means, we need to up sample the vector z until we get to an image that is 32 x 32 x 3 ,the same shape as our training imgages.

This may reminde you of the autoencoder lesson, in which we upsampled a comressed input using nearest neighbor interpolation or transpose convolutional layers. For this generator, we're going to use what was found successful in the original DCFAN model, transposed convolutions. 

To review, transposed convolutional layers are similar to the convolutions but with the opposite effect. With convolutional layers, you typically go from shallow and wide inputs to deep and narrow outputs, but with transposed convolutional layers, you go from narrow and deep inputs like vectors to wide and flat output like images. 
Consider using a layer with a stride of 2. When you move the convolutional kernel, one pixel in the input layer, the kernel will move two pixels in the output layer. So it's moving two pixels in the output layer for every one in the input layer and the output layer size depends on the stride. With a stride of 2, the transposed convolutional layer output will be twice the width and height of the input layer. 

Now, the first step in the generator is to connect the input vector Z to fully connected layer. Then we'll reshape that fully connected layer to a four by four XY shape with some depth that we specify. Then, we build a stack of largers by upsampling with transposed convolution. Each layer is doubled in XY size using stride of 2. Simulaneously, the depth is reduced as we move forward through these layers. The final layer, the output of the generator is a 32 x 32 x 3 convolutional layer, which is exactly what we want as the size of our generated images. For the generator, the authors of this original GAN paper use ReLU actiation and batch normalization on all hidden layers. The last layer is the only that should not have batchnormalzation. Instead, we'll just applying a tanh activation function to the output of the final layer. 
 
 
 
 ## What is Batch Normalization?
 
 
 ### What is Batch Normalization?
Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf). The idea is that, instead of just normalizing the inputs to the network, we normalize the inputs to every layer within the network.

#### Batch Normalization
It's called "batch" normalization because, during training, we normalize each layer's inputs by using the mean and standard deviation (or variance) of the values in the current batch. These are sometimes called the __batch statistics__.

> Specifically, batch normalization normalizes the output of a previous layer by subtracting the batch mean and dividing by the batch standard deviation.

Why might this help? Well, we know that normalizing the inputs to a network helps the network learn and converge to a solution. However, a network is a series of layers, where the output of one layer becomes the input to another. That means we can think of any layer in a neural network as the first layer of a smaller network.
 
 
 
 #### Normalization at Every Layer

For example, imagine a 3 layer network.

Instead of just thinking of it as a single network with inputs, layers, and outputs, think of the output of layer 1 as the input to a two layer network. This two layer network would consist of layers 2 and 3 in our original network.
 
Likewise, the output of layer 2 can be thought of as the input to a single layer network, consisting only of layer 3.
 
 
When you think of it like this - as a series of neural networks feeding into each other - then it's easy to imagine how normalizing the inputs to each layer would help. It's just like normalizing the inputs to any other neural network, but you're doing it at every layer (sub-network).

#### Internal Covariate Shift
Beyond the intuitive reasons, there are good mathematical reasons to motivate batch normalization. It helps combat what the authors call internal covariate shift.

> In this case, internal covariate shift refers to the change in the distribution of the inputs to different layers. It turns out that training a network is most efficient when the distribution of inputs to each layer is similar! 
 
 
 
And batch normalization is one method of standardizing the distribution of layer inputs. This discussion is best handled in the paper and in Deep Learning, a book you can read online written by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Specifically, check out the batch normalization section of Chapter 8: Optimization for Training Deep Models.


## Benefits of Batch Normalization

Batch_Normalization.ipynb

### Adding Batch Normalization Layers to a PyTorch Model
In the last notebook, you saw how a model with batch normalization applied reached a lower training loss and higher test accuracy! There are quite a few comments in that code, and I just want to recap a few of the most important lines.

To add batch normalization layers to a PyTorch model:

* You add batch normalization to layers inside the__init__ function.
* Layers with batch normalization do not include a bias term. So, for linear or convolutional layers, you'll need to set bias=False if you plan to add batch normalization on the outputs.
* You can use PyTorch's [BatchNorm1d] function to handle the math on linear outputs or [BatchNorm2d] for 2D outputs, like filtered images from convolutional layers.
* You add the batch normalization layer before calling the activation function, so it always goes layer > batch norm > activation.

Finally, when you tested your model, you set it to .eval() mode, which ensures that the batch normalization layers use the populationrather than the batch mean and variance (as they do during training).

 
 
 #### The takeaway
By using batch normalization to normalize the inputs at each layer of a network, we can make these inputs more consistent and thus reduce oscillations that may happen in gradient descent calculations. This helps us build deeper models that also converge faster!

Take a look at the [PyTorch BatchNorm2d documentation](https://pytorch.org/docs/stable/nn.html#batchnorm2d) to learn more about how to add batch normalization to a model, and how data is transformed during training (and evaluation).
 
 
### Benefits of Batch Normalization

Batch normalization optimizes network training. It has been shown to have several benefits:

1. __Networks train faster__ – Each training iteration will actually be slower because of the extra calculations during the forward pass and the additional hyperparameters to train during back propagation. However, it should converge much more quickly, so training should be faster overall.

2. __Allows higher learning rates__ – Gradient descent usually requires small learning rates for the network to converge. And as networks get deeper, their gradients get smaller during back propagation so they require even more iterations. Using batch normalization allows us to use much higher learning rates, which further increases the speed at which networks train.

3. __Makes weights easier to initialize__ – Weight initialization can be difficult, and it's even more difficult when creating deeper networks. Batch normalization seems to allow us to be much less careful about choosing our initial starting weights.

4. __Makes more activation functions viable__ – Some activation functions do not work well in some situations. Sigmoids lose their gradient pretty quickly, which means they can't be used in deep networks. And ReLUs often die out during training, where they stop learning completely, so we need to be careful about the range of values fed into them. Because batch normalization regulates the values going into each activation function, non-linearlities that don't seem to work well in deep networks actually become viable again.

5. __Simplifies the creation of deeper networks__ – Because of the first 4 items listed above, it is easier to build and faster to train deeper neural networks when using batch normalization. And it's been shown that deeper networks generally produce better results, so that's great.

6. __Provides a bit of regularization__ – Batch normalization adds a little noise to your network. In some cases, such as in Inception modules, batch normalization has been shown to work as well as dropout. But in general, consider batch normalization as a bit of extra regularization, possibly allowing you to reduce some of the dropout you might add to a network.

7.__May give better results overall__ – Some tests seem to show batch normalization actually improves the training results. However, it's really an optimization to help train faster, so you shouldn't think of it as a way to make your network better. But since it lets you train networks faster, that means you can iterate over more designs more quickly. It also lets you build deeper networks, which are usually better. So when you factor in everything, you're probably going to end up with better results if you build your networks with batch normalization.
 
 
 ## DCGAN Notebook & Data
 
 DCGAN_Exercise.ipynb
 
 
 #### Why no bias?
The reason there is no bias for our convolutional layers is because we have batch normalization applied to their outputs. The goal of batch normalization is to get outputs with:

* mean = 0
* standard deviation = 1

Since we want the mean to be 0, we do not want to add an offset (bias) that will deviate from 0. We want the outputs of our convolutional layer to rely only on the coefficient weights.
 
 
 
### GANs for Illuminating Model Weaknesses
GANs are not only used for image generation, they are also used to find weaknesses in existing, trained models. The adversarial examples that a generator learns to make, can be designed to trick a pre-trained model. Essentially, small perturbations in images can cause a classifier (like AlexNet or a known image classifier) to fail pretty spectacularly!

> [This OpenAI blog post](https://openai.com/blog/adversarial-example-research/) details how adversarial examples can be used to "attack" existing models, and discusses potential security issues. And one example of a perturbation that causes misclassification can be seen below.
 
 
 
 
 
 
## Other Interesting Applications of GANs
 
So far, you've seen a lot of examples of how GANs might be used for image generation and transformation. GANs are a relatively new formulation and so there are some really exciting research directions that include GANs. I didn't have time to cover them all in video, so I wanted to highlight a few of my favorite examples, here, and link to some resources that I've found helpful! This page is for those who are interested in learning more about GANs and curious to learn about semi-supervised learning.
 


### 1. Semi-Supervised Learning
Semi-supervised models are used when you only have a few labeled data points. The motivation for this kind of model is that, we increasingly have a lot of raw data, and the task of labelling data is tedious, time-consuming, and often, sensitive to human error. Semi-supervised models give us a way to learn from a large set of data with only a few labels, and they perform surprisingly well even though the amount of labeled data you have is relatively tiny. Ian Goodfellow has put together a video on this top, which you can see, below.

 #### Semi-Supervised Learning in PyTorch
There is a readable implementation of a semi-supervised GAN in [this Github repository](https://github.com/Sleepychord/ImprovedGAN-pytorch). If you'd like to implement this in code, I suggest reading through that code!
 
 
 ### 2. Domain Invariance
Consider [this car classification example](https://arxiv.org/abs/1709.02480). From the abstract, researchers (Timnit Gebru, et. al) wanted to:

> develop a computer vision pipeline to predict income, per capita carbon emission, crime rates and other city attributes from a single source of publicly available visual data. We first detect cars in 50 million images across 200 of the largest US cities and train a model to predict demographic attributes using the detected cars. To facilitate our work, we have collected the largest and most challenging fine-grained dataset reported to date consisting of over 2600 classes of cars comprised of images from Google Street View and other web sources, classified by car experts to account for even the most subtle of visual differences.

One interesting thing to note is that these researchers obtained some manually-labeled Streetview data and data from other sources. I'll call these image sources, domains. So Streetview is a domain and another source, say cars.com is separate domain.
 
 
The researchers then had to find a way to combine what they learned from these multiple sources! They did this with the use of multiple classifiers; adversarial networks that do not include a Generator, just two classifiers.

> * One classifier is learning to recognize car types
> 
> * And another is learning to classify whether a car image came from Google Streetview or cars.com, given the extracted features from that image

So, the first classier’s job is to classify the car image correctly and to trick the second classifier so that the second classifier cannot tell whether the extracted image features indicate an image from the Streetview or cars.com domain!

The idea is: if the second classifier cannot tell which domain the features are from, then this indicates that these features are shared among the two domains, and you’ve found features that are domain-invariant.
 
Domain-invariance can be applied to a number of applications in which you want to find features that are invariant between two different domains. These can be image domains or domains based on different population demographics and so on. This is also sometimes referred to as [adversarial feature learning](https://arxiv.org/pdf/1705.11122.pdf).
 


### 3. Ethical and Artistic Applications: Further Reading

 * [Ethical implications of GANs](https://www.newyorker.com/magazine/2018/11/12/in-the-age-of-ai-is-seeing-still-believing) and when "fake" images can give us information about reality.

 * [Do Androids Dream in Balenciaga](https://www.ssense.com/en-us/editorial/fashion/do-androids-dream-of-balenciaga-ss29)? note that the author briefly talks about generative models having artistic potential rather than ethical implications, but the two go hand in hand. The generator, in this case, will recreate what it sees on the fashion runway; typically thin, white bodies that do not represent the diversity of people in the world (or even the diversity of people who buy Balenciaga).
 
 
 
 
 
 
 
