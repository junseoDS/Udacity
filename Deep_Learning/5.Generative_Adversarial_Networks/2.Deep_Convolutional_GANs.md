# Deep Convolutional GANs


## Deep Convolutional GANs

In this lesson we will be training to recognize numbers and photos of house addresses. 

### Street View House Number (SVHN) Dataset 
The numbers are of different colors and in differnet settings to MNIST

To get the best performance out of our model, we're going to use convolutional layers to make a deep convolutional GAN, usually called DCGAN. 
A DCGAN will still have a generator and discriminator networks, only this time these networks we made of convolutional layers that are designed to work with spatial data. The discriminator will be a kind of convolutional neural network that aims to classify any image it sees as either real training image or a fake generated image, and generator will be a transpose convolutional network that aims to upsample a laten vector z and generate images that can fool the discriminator. These networks have opposing goals and they're pulling the error rate of discriminator in different directions. 



 
 ## DCGAN, Discriminator
 
 A DCGAN is made of two networks, a generator and a discriminator.  
 
 ### Discriminator
 
 The discriminator is a convolutional neural network with one fully connected layer at the end. They should take in an image is input and output of value that indicates whether that image is real and from our training data or fake. There are __no maxpooling layers in this network__. Instead, the down sampling is done entirely with convolutional layers that have a stride equal to 2. 
 

 
 ### DCGAN Paper
It's always good to take a look at the original paper when you can. Many papers discuss both the theory and training details of deep learning networks, and you can read the DCGAN paper, [Unsupervised Representational Learning with Deep Convolutional Generative Adversarial Networks, at this link](https://arxiv.org/pdf/1511.06434.pdf).
 
 
 
 #### Architecture guidelines for stable Deep Convolutional GANs

 * Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions(generator)
 * Use batchnorm in both the generator and the discriminator.
 * Remove fully connected hidden layers for deeper architectures.
 * Use ReLU activation in generator for all layers except for the output, which uses Tanh.
 * Use LeakyReLu activation in the discriminator for all layers.
 
 
 
 
 
 ## DCGAN Generator
 
The generator is, I think the most interesting part of a GAN because its job is to really learn something about the underlying structure of training data. 
 
As before, the input to the generator is a random vector z, usually with somthing around a 100 values. The output of generator, it needs to be an image that can be sent to the discriminator. That means, we need to up sample the vector z until we get to an image that is 32 x 32 x 3 ,the same shape as our training imgages.

This may reminde you of the autoencoder lesson, in which we upsampled a comressed input using nearest neighbor interpolation or transpose convolutional layers. For this generator, we're going to use what was found successful in the original DCFAN model, transposed convolutions. 

To review, transposed convolutional layers are similar to the convolutions but with the opposite effect. With convolutional layers, you typically go from shallow and wide inputs to deep and narrow outputs, but with transposed convolutional layers, you go from narrow and deep inputs like vectors to wide and flat output like images. 
Consider using a layer with a stride of 2. When you move the convolutional kernel, one pixel in the input layer, the kernel will move two pixels in the output layer. So it's moving two pixels in the output layer for every one in the input layer and the output layer size depends on the stride. With a stride of 2, the transposed convolutional layer output will be twice the width and height of the input layer. 

Now, the first step in the generator is to connect the input vector Z to fully connected layer. Then we'll reshape that fully connected layer to a four by four XY shape with some depth that we specify. Then, we build a stack of largers by upsampling with transposed convolution. Each layer is doubled in XY size using stride of 2. Simulaneously, the depth is reduced as we move forward through these layers. The final layer, the output of the generator is a 32 x 32 x 3 convolutional layer, which is exactly what we want as the size of our generated images. For the generator, the authors of this original GAN paper use ReLU actiation and batch normalization on all hidden layers. The last layer is the only that should not have batchnormalzation. Instead, we'll just applying a tanh activation function to the output of the final layer. 
 
 
 
 ## What is Batch Normalization?
 
 
 ### What is Batch Normalization?
Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf). The idea is that, instead of just normalizing the inputs to the network, we normalize the inputs to every layer within the network.

#### Batch Normalization
It's called "batch" normalization because, during training, we normalize each layer's inputs by using the mean and standard deviation (or variance) of the values in the current batch. These are sometimes called the __batch statistics__.

> Specifically, batch normalization normalizes the output of a previous layer by subtracting the batch mean and dividing by the batch standard deviation.

Why might this help? Well, we know that normalizing the inputs to a network helps the network learn and converge to a solution. However, a network is a series of layers, where the output of one layer becomes the input to another. That means we can think of any layer in a neural network as the first layer of a smaller network.
 
 
 
 #### Normalization at Every Layer

For example, imagine a 3 layer network.

Instead of just thinking of it as a single network with inputs, layers, and outputs, think of the output of layer 1 as the input to a two layer network. This two layer network would consist of layers 2 and 3 in our original network.
 
Likewise, the output of layer 2 can be thought of as the input to a single layer network, consisting only of layer 3.
 
 
When you think of it like this - as a series of neural networks feeding into each other - then it's easy to imagine how normalizing the inputs to each layer would help. It's just like normalizing the inputs to any other neural network, but you're doing it at every layer (sub-network).

#### Internal Covariate Shift
Beyond the intuitive reasons, there are good mathematical reasons to motivate batch normalization. It helps combat what the authors call internal covariate shift.

> In this case, internal covariate shift refers to the change in the distribution of the inputs to different layers. It turns out that training a network is most efficient when the distribution of inputs to each layer is similar! 
 
 
 
And batch normalization is one method of standardizing the distribution of layer inputs. This discussion is best handled in the paper and in Deep Learning, a book you can read online written by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Specifically, check out the batch normalization section of Chapter 8: Optimization for Training Deep Models.


## Benefits of Batch Normalization

Batch_Normalization.ipynb

### Adding Batch Normalization Layers to a PyTorch Model
In the last notebook, you saw how a model with batch normalization applied reached a lower training loss and higher test accuracy! There are quite a few comments in that code, and I just want to recap a few of the most important lines.

To add batch normalization layers to a PyTorch model:

You add batch normalization to layers inside the__init__ function.
Layers with batch normalization do not include a bias term. So, for linear or convolutional layers, you'll need to set bias=False if you plan to add batch normalization on the outputs.
You can use PyTorch's [BatchNorm1d] function to handle the math on linear outputs or [BatchNorm2d] for 2D outputs, like filtered images from convolutional layers.
You add the batch normalization layer before calling the activation function, so it always goes layer > batch norm > activation.
Finally, when you tested your model, you set it to .eval() mode, which ensures that the batch normalization layers use the populationrather than the batch mean and variance (as they do during training).

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
